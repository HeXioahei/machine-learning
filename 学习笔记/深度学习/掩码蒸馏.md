# 掩码生成式蒸馏
**摘要**：知识蒸馏已成功应用于各种任务。当前的蒸馏算法通常通过模仿教师的输出来提高学生的性能。本文表明，教师也可以通过引导学生进行特征恢复来提高学生的表征能力。基于此，我们提出了掩码生成式蒸馏（MGD），方法很简单：我们掩盖学生特征中的随机像素，并通过一个简单的模块强制其生成教师的完整特征。MGD是一种真正通用的基于特征的蒸馏方法，可用于各种任务，包括图像分类、目标检测、语义分割和实例分割。我们在不同的模型和广泛的数据集上进行了实验，结果表明所有学生都取得了显著的改进。值得注意的是，我们将ResNet - 18在ImageNet上的top - 1准确率从69.90%提高到71.69%，将基于ResNet - 50骨干网的RetinaNet的边界框平均精度（mAP）从37.4提高到41.0，将基于ResNet - 50的SOLO的掩码平均精度（Mask mAP）从33.1提高到36.2，将基于ResNet - 18的DeepLabV3的平均交并比（mIoU）从73.20提高到76.02。我们的代码可在https://github.com/yzd - v/MGD获取。 
**关键词**：知识蒸馏；图像分类；目标检测；语义分割；实例分割 
## 1. 引言
深度卷积神经网络（CNNs）已广泛应用于各种计算机视觉任务。一般来说，较大的模型性能更好，但推理速度较慢，在资源有限的情况下难以部署。为了解决这个问题，知识蒸馏被提出。根据蒸馏的位置，它可以分为两类。第一类是为不同任务专门设计的，例如用于分类的基于logit的蒸馏和用于检测的基于头部的蒸馏。第二类是基于特征的蒸馏。由于不同网络之间只有特征之后的头部或投影器不同，理论上，基于特征的蒸馏方法可以用于各种任务。然而，为特定任务设计的蒸馏方法通常不适用于其他任务。例如，OFD和KR对检测器的改进有限。专门为检测器设计的FKD和FGD由于缺乏颈部结构，无法用于其他任务。 先前的基于特征的蒸馏方法通常让学生尽可能地模仿教师的输出，因为教师的特征具有更强的表征能力。
然而，我们认为没有必要直接模仿教师来提高学生特征的表征能力。用于蒸馏的特征通常是通过深度网络得到的高阶语义信息。特征像素在一定程度上已经包含了相邻像素的信息。因此，如果我们能够通过一个简单的模块使用部分像素来恢复教师的完整特征，那么这些使用的像素的表征能力也可以得到提高。基于此，我们提出了掩码生成式蒸馏（MGD），这是一种简单而高效的基于特征的蒸馏方法。如图2所示，我们首先掩盖学生特征中的随机像素，然后通过一个简单的模块用被掩盖的特征生成教师的完整特征。由于在每次迭代中都使用随机像素，在整个训练过程中所有像素都会被使用，这意味着特征将更加鲁棒，其表征能力也将得到提高。在我们的方法中，教师仅作为学生恢复特征的指导，而不要求学生直接模仿它。 
为了证实我们的假设，即不直接模仿教师，通过掩码特征生成可以提高学生的特征表征能力，我们对学生和教师的颈部特征注意力进行了可视化。如图1所示，学生和教师的特征有很大不同。与教师相比，学生特征的背景有更高的响应。教师的mAP也显著高于学生，分别为41.0和37.4。使用最先进的蒸馏方法FGD（该方法强制学生通过注意力模仿教师的特征）进行蒸馏后，学生的特征变得更类似于教师的特征，mAP大幅提高到40.7。而使用MGD训练后，学生和教师的特征仍然存在显著差异，但学生对背景的响应大大降低。我们还惊讶地发现，学生的性能超过了FGD，甚至达到了与教师相同的mAP。这也表明使用MGD训练可以提高学生特征的表征能力。此外，我们还在图像分类和密集预测任务上进行了大量实验。结果表明，MGD可以为各种任务带来显著的改进，包括图像分类、目标检测、语义分割和实例分割。MGD还可以与其他基于logit或基于头部的蒸馏方法结合使用，以获得更大的性能提升。综上所述，本文的贡献如下： 1. 我们引入了一种基于特征的知识蒸馏的新方法，该方法让学生用其掩码特征生成教师的特征，而不是直接模仿。 2. 我们提出了一种新的基于特征的蒸馏方法，即掩码生成式蒸馏（MGD），它简单易用，只有两个超参数。 3. 我们通过在不同数据集上对各种模型进行广泛的实验，验证了我们方法的有效性。对于图像分类和密集预测任务，学生使用MGD都取得了显著的改进。 
## 2. 相关工作 
### 2.1 用于分类的知识蒸馏 
知识蒸馏最早由Hinton等人提出，其中学生由教师最后一个线性层的标签和软标签进行监督。然而，除了logit之外，更多的蒸馏方法是基于特征图的。FitNet从中间层提取语义信息。AT总结了通道维度上的值，并将注意力知识转移给学生。OFD提出了边缘ReLU，并设计了一个新的函数来测量蒸馏的距离。CRD利用对比学习将知识转移给学生。最近，KR建立了一个回顾机制，并利用多层次信息进行蒸馏。SRRL将表示学习和分类解耦，利用教师的分类器训练学生的倒数第二层特征。WSLD从偏差 - 方差权衡的角度提出了用于蒸馏的加权软标签。
### 2.2 用于密集预测的知识蒸馏 
分类和密集预测之间存在很大差异。许多用于分类的蒸馏方法在密集预测上失败了。理论上，基于特征的蒸馏方法应该对分类和密集预测任务都有帮助，这也是我们方法的目标。 用于目标检测的知识蒸馏。Chen等人首先在检测器的颈部和头部计算蒸馏损失。由于前景和背景之间的极度不平衡，目标检测蒸馏的关键在于在哪里进行蒸馏。为了避免引入背景噪声，FGFI利用细粒度掩码对物体附近的区域进行蒸馏。然而，Defeat指出前景和背景的信息都很重要。GID选择学生和教师表现不同的区域进行蒸馏。FKD使用教师和学生的注意力之和，使学生关注可变区域。FGD提出了焦点蒸馏，强制学生学习教师的关键部分，并进行全局蒸馏以补偿缺失的全局信息。 用于语义分割的知识蒸馏。Liu等人提出了成对和整体蒸馏，强制学生和教师输出之间的成对和高阶一致性。He等人将教师网络的输出重新解释为一个重新表示的潜在域，并从教师网络中捕获长期依赖关系。CWD通过最小化由每个通道的激活图归一化计算得到的概率图之间的Kullback - Leibler散度来进行蒸馏。 
## 3. 方法
不同任务的模型架构差异很大。此外，大多数蒸馏方法是为特定任务设计的。然而，基于特征的蒸馏可以应用于分类和密集预测。基于特征蒸馏的基本方法可以表述为：$L_{f e a}=\sum_{k=1}^{C} \sum_{i=1}^{H} \sum_{j=1}^{W}\left(F_{k, i, j}^{T}-f_{align }\left(F_{k, i, j}^{S}\right)\right)^{2}$，其中$F^{T}$和$F^{S}$分别表示教师和学生的特征，$f_{align}$是用于将学生特征$F^{S}$与教师特征$F^{T}$对齐的适配层，$C$、$H$、$W$表示特征图的形状。 这种方法帮助学生直接模仿教师的特征。然而，我们提出了掩码生成式蒸馏（MGD），旨在强制学生生成教师的特征而不是模仿它，从而在分类和密集预测方面为学生带来显著的改进。MGD的架构如图2所示，我们将在本节中详细介绍。 
### 3.1 用掩码特征生成
对于基于CNN的模型，更深层的特征具有更大的感受野，能更好地表示原始输入图像。换句话说，特征图像素在一定程度上已经包含了相邻像素的信息。因此，我们可以使用部分像素来恢复完整的特征图。我们的方法旨在通过学生的掩码特征生成教师的特征，这可以帮助学生实现更好的表示。 我们分别用$T^{l} \in R^{C ×H ×W}$和$S^{l} \in R^{C ×H ×W}(l = 1,..., L)$表示教师和学生的第$l$个特征图。首先，我们设置第$l$个随机掩码来覆盖学生的第$l$个特征，其公式为：$M_{i, j}^{l}= \begin{cases}0, & if R_{i, j}^{l}<\lambda \\ 1, & Otherwise \end{cases}$，其中$R_{i, j}^{l}$是(0, 1)中的一个随机数，$i$、$j$分别是特征图的水平和垂直坐标。$\lambda$是一个超参数，表示掩码比例。第$l$个特征图被第$l$个随机掩码覆盖。 然后，我们使用相应的掩码覆盖学生的特征图，并尝试用剩余的像素生成教师的特征图，公式如下：$\mathcal{G}\left(f_{align }\left(S^{l}\right) \cdot M^{l}\right) \to T^{l}$，$\mathcal{G}(F)=W_{l 2}\left(ReLU\left(W_{l 1}(F)\right)\right)$。 $\mathcal{G}$表示投影层，它包括两个卷积层$W_{l 1}$和$W_{12}$以及一个激活层$ReLU$。在本文中，我们对适配层$f_{align}$采用$1×1$卷积层，对投影层$W_{l 1}$和$W_{12}$采用$3×3$卷积层。 根据这种方法，我们为MGD设计了蒸馏损失$L_{dis}$：$L_{dis}(S, T)=\sum_{l=1}^{L} \sum_{k=1}^{C} \sum_{i=1}^{H} \sum_{j=1}^{W}\left(T_{k, i, j}^{l}-\mathcal{G}\left(f_{align }\left(S_{k, i, j}^{l}\right) \cdot M_{i, j}^{l}\right)\right)^{2}$，其中$L$是用于蒸馏的层数总和，$C$、$H$、$W$表示特征图的形状，$S$和$T$分别表示学生和教师的特征。 
### 3.2 总体损失
使用提出的MGD蒸馏损失$L_{dis}$，我们用以下总损失训练所有模型：$L_{all }=L_{original }+\alpha \cdot L_{dis}$，其中$L_{original}$是所有任务中模型的原始损失，$\alpha$是一个用于平衡损失的超参数。 MGD是一种简单有效的蒸馏方法，可以很容易地应用于各种任务。我们的方法的过程总结在算法1中。 
**算法1 掩码生成式蒸馏** 
输入：教师$T$，学生$S$，输入$x$，标签$y$，超参数$\alpha$，$\lambda$ 
1. 使用$S$得到输入$x$的特征$feaS$和输出$\hat{y}$ 
2. 使用$T$得到输入$x$的特征$feaT$ 
3. 计算模型的原始损失：$L_{original}(\hat{y}, y)$ 
4. 计算公式5中的蒸馏损失：$L_{dis}(feaS, feaT)$ 
5. 使用$L_{all }=L_{original }+\alpha \cdot L_{dis}$更新$S$ 输出：$S$ 
## 4. 主要实验
MGD是一种基于特征的蒸馏方法，可以很容易地应用于不同模型的各种任务。在本文中，我们在各种任务上进行了实验，包括分类、目标检测、语义分割和实例分割。我们针对不同的任务使用不同的模型和数据集进行实验，所有模型使用MGD都取得了优异的改进。 
### 4.1 分类 
**数据集**：对于分类任务，我们在ImageNet上评估我们的知识蒸馏方法，该数据集包含1000个对象类别。在所有分类实验中，我们使用120万张图像进行训练，5万张图像进行测试。我们使用准确率来评估模型。 
**实现细节**：对于分类任务，我们在骨干网的最后一个特征图上计算蒸馏损失。关于这一点的消融研究在5.5节中展示。MGD使用一个超参数$\alpha$来平衡公式6中的蒸馏损失。另一个超参数$\lambda$用于调整公式2中的掩码比例。在所有分类实验中，我们采用超参数$\alpha = 7×10^{-5}$，$\lambda = 0.5$。我们使用随机梯度下降（SGD）优化器训练所有模型100个epoch，其中动量为0.9，权重衰减为0.0001。我们将学习率初始化为0.1，并每30个epoch衰减一次。此设置基于8个GPU。实验是使用基于Pytorch的MMClassification和MMRazor进行的。 
**分类结果**：我们使用两种流行的分类蒸馏设置进行实验，包括同质蒸馏和异质蒸馏。第一种蒸馏设置是从ResNet - 34到ResNet - 18，另一种设置是从ResNet - 50到MobileNet。如表1所示，我们与各种知识蒸馏方法进行了比较，包括基于特征的方法、基于logit的方法以及它们的组合。使用我们的方法，学生ResNet - 18和MobileNet的Top - 1准确率分别提高了1.68和3.14。此外，如前所述，MGD只需要在特征图上计算蒸馏损失，并且可以与其他基于logit的方法结合用于图像分类。因此，我们尝试添加WSLD中的基于logit的蒸馏损失。这样，两个学生的Top - 1准确率分别达到了71.80和72.59，又分别提高了0.22和0.24。 
### 4.2 目标检测和实例分割 
**数据集**：我们在COCO2017数据集上进行实验，该数据集包含80个对象类别。我们使用12万张训练图像进行训练，5千张验证图像进行测试。模型的性能用平均精度（Average Precision）来评估。 
**实现细节**：我们在颈部的所有特征图上计算蒸馏损失。对于所有的一阶段模型，我们采用超参数$\{\alpha = 2×10^{-5}, \lambda = 0.65\}$，对于所有的二阶段模型，我们采用超参数$\{\alpha = 5×10^{-7}, \lambda = 0.45\}$。我们使用SGD优化器训练所有模型，其中动量为0.9，权重衰减为0.0001。除非另有说明，我们训练模型24个epoch。当学生和教师具有相同的头部结构时，我们使用继承策略，用教师的颈部和头部参数初始化学生来训练学生。实验是使用MMDetection进行的。 
**目标检测和实例分割结果**：对于目标检测，我们在三种不同类型的检测器上进行实验，包括一个二阶段检测器（Faster RCNN）、一个基于锚点的一阶段检测器（RetinaNet）和一个无锚点的一阶段检测器（RepPoints）。我们将MGD与三种最新的用于检测器的最先进蒸馏方法进行了比较。对于实例分割，我们在两个模型SOLO和Mask RCNN上进行实验。如表2和表3所示，我们的方法在目标检测和实例分割方面都优于其他最先进的方法。使用MGD，学生在COCO数据集上的平均精度有了显著提高，例如，基于ResNet - 50的RetinaNet和SOLO的边界框mAP分别提高了3.6，掩码mAP分别提高了3.1。
### 4.3 语义分割 
**数据集**：对于语义分割任务，我们在CityScapes数据集上评估我们的方法，该数据集包含5000张高质量图像（其中2975张用于训练，500张用于验证，1525张用于测试）。我们用平均交并比（mIoU）来评估所有模型。 
**实现细节**：对于所有模型，我们在骨干网的最后一个特征图上计算蒸馏损失。在所有实验中，我们采用超参数$\{\alpha = 2×10^{-5}, \lambda = 0.75\}$。我们使用SGD优化器训练所有模型，其中动量为0.9，权重衰减为0.0005。我们在8个GPU上运行所有模型。实验是使用MMSegmentation进行的。 
**语义分割结果**：对于语义分割任务，我们在两种设置下进行实验。在这两种设置中，我们都使用PspNet - Res101作为教师，并以$512×1024$的输入大小训练80k次迭代。我们使用PspNet - Res18和DeepLabV3 - Res18作为学生，并以$512×1024$的输入大小训练40k次迭代。如表4所示，我们的方法优于用于语义分割的最先进蒸馏方法。同质和异质蒸馏都为学生带来了显著的改进，例如，基于ResNet - 18的PspNet的mIoU提高了3.78。此外，MGD是一种基于特征的蒸馏方法，可以与其他基于logit的蒸馏方法结合使用。结果表明，通过添加CWD中头部的logit蒸馏损失，学生PspNet和DeepLabV3的mIoU又分别提高了0.47和0.29。 
## 5. 分析
### 5.1 MGD带来更好的表征
MGD强制学生用其掩码特征生成教师的完整特征图，而不是直接模仿它。这有助于学生更好地表示输入图像。在本小节中，我们通过让学生自我教学来研究这一点。我们首先直接将ResNet - 18训练为教师和基线。然后，我们使用训练好的ResNet - 18通过MGD对自身进行蒸馏。为了进行比较，我们还通过强制学生直接模仿教师来对学生进行蒸馏。模仿的蒸馏损失是学生特征图与教师特征图之间的L2距离的平方。 如表5所示，即使教师是自身，学生使用MGD也能获得1.01的准确率提升。相比之下，当强制学生直接模仿教师的特征图时，提升非常有限。这种比较表明，蒸馏后学生的特征图比教师的特征图具有更好的表征能力。 此外，我们可视化了使用MGD蒸馏和模仿教师的训练损失曲线，如图3所示。图中的差异表示学生和教师最后一个特征图之间的L2距离的平方，这也是模仿教师的蒸馏损失。如图所示，在直接模仿教师时，差异不断减小，最终学生获得了与教师相似的特征。然而，这种方法的改进很小。相比之下，使用MGD训练后差异变大。虽然学生获得了与教师不同的特征，但它获得了更高的准确率，这也表明学生的特征获得了更强的表征能力。 
### 5.2 通过掩码随机通道进行蒸馏 
对于图像分类，模型通常使用池化层来降低特征图的空间维度。这一层使得模型对通道比空间像素更敏感。因此，在本小节中，我们尝试通过掩码随机通道而不是空间像素来应用MGD进行分类。我们在实验中采用掩码比例$\beta = 0.15$和超参数$\alpha = 7×10^{-5}$。如表6所示，对于图像分类，通过掩码随机通道，学生可以获得更好的性能。学生Res - 18和MobileNet的Top - 1准确率分别提高了0.13和0.14。
### 5.3 使用不同的教师进行蒸馏
Cho等人表明，在图像分类的知识蒸馏中，准确率更高的教师不一定是更好的教师。这个结论是基于基于logit的蒸馏方法得出的。然而，我们的方法只需要在特征图上计算蒸馏损失。在本小节中，我们通过使用不同种类的教师对同一个学生ResNet - 18进行蒸馏来研究这个结论，如图4所示。 如图4所示，当教师和学生具有相似的架构时，更好的教师对学生更有益，例如，以ResNet - 18和ResNetV1D - 152作为教师时，ResNet - 18分别达到了70.91和71.8的准确率。然而，当教师和学生具有不同的架构时，学生很难生成教师的特征图，蒸馏带来的改进也有限。此外，架构差异越大，蒸馏效果越差。例如，尽管Res2Net101和ConvNeXt - T的准确率分别为79.19和82.05，但它们仅为学生带来了1.53和0.88的准确率提升，甚至低于基于ResNet - 34的教师（准确率为73.62）。 图4中的结果表明，当教师和学生具有相似的架构时，更强的教师对于基于特征的蒸馏更好。此外，同质教师对于基于特征的蒸馏比准确率高但架构异构的教师要好得多。 
### 5.4 生成块
MGD使用一个简单的块来恢复特征，称为生成块。在公式4中，我们使用两个$3×3$卷积层和一个激活层ReLU来实现这一点。在本小节中，我们探索了不同组成的生成块的效果，如表7所示。 结果表明，当只有一个卷积层时，学生的改进最小。然而，当有三个卷积层时，学生的Top - 1准确率较差，但Top - 5准确率较好。至于卷积核大小，$5×5$卷积核需要更多的计算资源，但性能更差。基于这些结果，我们为MGD选择了公式4中的架构，它包括两个卷积层和一个激活层。 
### 5.5 在不同阶段进行蒸馏
我们的方法也可以应用于模型的其他阶段。在本小节中，我们在ImageNet上探索了在不同阶段进行蒸馏的情况。我们在教师和学生的相应层上计算蒸馏损失。如表8所示，蒸馏较浅的层对学生也有帮助，但非常有限。而蒸馏包含更多语义信息的较深层次对学生更有益。此外，早期阶段的特征不直接用于分类。因此，将这些特征与最后阶段的特征一起蒸馏可能会损害学生的准确率。 
### 5.6 超参数的敏感性研究 
在本文中，我们使用公式6中的$\alpha$和公式2中的$\lambda$分别来平衡蒸馏损失和调整掩码比例。在本小节中，我们通过在ImageNet数据集上使用ResNet - 34对ResNet - 18进行蒸馏来对超参数进行敏感性研究。结果如图5所示。 如图5所示，MGD对仅用于平衡损失的超参数$\alpha$不敏感。至于掩码比例$\lambda$，当它为0时，准确率为71.41，这意味着没有掩码部分用于生成。当$\lambda < 0.5$时，学生随着比例的增大获得更高的性能。然而，当$\lambda$太大，例如0.8时，剩余的语义信息太贫乏，无法生成教师的完整特征图，性能提升也会受到影响。 
## 6. 结论 
在本文中，我们提出了一种新的知识蒸馏方法，该方法强制学生用其掩码特征生成教师的特征，而不是直接模仿。基于此方法，我们提出了一种新的知识蒸馏方法，即掩码生成式蒸馏（MGD）。使用MGD，学生可以获得更强的表征能力。此外，我们的方法仅基于特征图，因此MGD可以很容易地应用于各种任务，如图像分类、目标检测、语义分割和实例分割。在不同数据集上对各种模型进行的大量实验证明了我们的方法简单且高效。