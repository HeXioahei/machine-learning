# 第一轮实验

**模型解释：**

* **vit_base_patch16_224_in21k**：原论文中ViT-B/16模型的ImageNet-21k版本，其参数量为11.8M。
* **vit_base_patch16_224**：原论文中提到的ViT-B/16模型，是一个基础模型，其参数量为10M。使用16×16的图像块（patch）大小，并在ImageNet-1k数据集上以224×224的输入尺寸进行预训练。       
* **vit_base_patch32_224**：原论文中提到的ViT-B/32模型，是一个基础模型，其参数量为15M。使用32×32的图像块（patch）大小，并在ImageNet-1k数据集上以224×224的输入尺寸进行预训练。
* **vit_large_patch16_224**：原论文中的ViT-L/16模型，是一个大模型，其参数量为30M。使用16×16的图像块（patch）大小，并在ImageNet-1k数据集上以224×224的输入尺寸进行预训练。

**训练结果和验证结果对比：**

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202411162227445.png)

对比可见，后三个模型的训练结果很好，但是可能有一点点过拟合了，所以loss_valid会比loss_train高一点点，acc会低一点点。

**同一张图片的不同模型预测对比示例：**
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202411162249685.png)

这一张图片很奇怪，最后一模型预测出来的结果竟然直接变成了rose。我们在想可能是因为权重参数量太大，导致了严重的过拟合。


![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202411162250130.png)

这一张图片的效果就很好，虽然是模糊图片，但最终的预测效果不断变好，甚至接近到了0.999。