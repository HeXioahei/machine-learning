# torch.max()
[详解 torch.max 函数_torch.max函数-CSDN博客](https://blog.csdn.net/ViatorSun/article/details/108909312)

# optimizer.step()

# softmax(dim=)
dim=-1，对最后一个维度进行softmax，对于打印出来呈现的效果就是，对每一行进行softmax，每一行之和为1。
dim=-2，对倒数第二个维度进行softmax，对于打印出来呈现的效果就是，对每一列进行softmax，每一列之和为1。
dim=-3，对倒数第三个维度进行softmax，对于打印出来呈现的效果就是，看下面的效果吧。
dim=0，对第一个维度进行softmax，对于打印出来呈现的效果就是，看下面的效果吧。

```
>>> a = torch.randn(2,2,3,3)
>>> a
tensor([[[[-0.2953, -1.4610, -0.2881],
          [-1.7084, -2.2948, -0.9451],
          [-0.8563, -1.3055,  0.4355]],

         [[ 0.4595,  0.0872,  0.4769],
          [ 0.6064, -0.1889, -1.3931],
          [-0.1660,  0.3209, -0.6197]]],


        [[[ 0.7242, -1.2179, -0.2090],
          [ 0.1714, -1.0020,  0.3806],
          [-1.7505,  0.6091,  1.5457]],

         [[ 1.0053,  2.3407,  2.2515],
          [ 0.5637, -0.1457, -0.6264],
          [ 0.7995,  1.1969, -0.0315]]]])
>>> a.softmax(dim=-1)
tensor([[[[0.4312, 0.1344, 0.4343],
          [0.2701, 0.1503, 0.5796],
          [0.1895, 0.1209, 0.6896]],

         [[0.3695, 0.2546, 0.3759],
          [0.6302, 0.2845, 0.0853],
          [0.3065, 0.4988, 0.1947]]],


        [[[0.6507, 0.0933, 0.2559],
          [0.3934, 0.1217, 0.4849],
          [0.0259, 0.2743, 0.6998]],

         [[0.1208, 0.4592, 0.4200],
          [0.5568, 0.2739, 0.1694],
          [0.3421, 0.5089, 0.1490]]]])
>>> a.softmax(dim=-2)
tensor([[[[0.5513, 0.3842, 0.2793],
          [0.1342, 0.1669, 0.1448],
          [0.3146, 0.4489, 0.5759]],

         [[0.3713, 0.3309, 0.6720],
          [0.4300, 0.2511, 0.1036],
          [0.1986, 0.4180, 0.2244]]],


        [[[0.6026, 0.1183, 0.1165],
          [0.3467, 0.1468, 0.2101],
          [0.0507, 0.7350, 0.6735]],

         [[0.4070, 0.7134, 0.8634],
          [0.2617, 0.0594, 0.0486],
          [0.3313, 0.2273, 0.0880]]]])
>>> a.softmax(dim=-3)
tensor([[[[0.3198, 0.1754, 0.3176],
          [0.0899, 0.1085, 0.6101],
          [0.3340, 0.1643, 0.7418]],

         [[0.6802, 0.8246, 0.6824],
          [0.9101, 0.8915, 0.3899],
          [0.6660, 0.8357, 0.2582]]],


        [[[0.4302, 0.0277, 0.0787],
          [0.4032, 0.2981, 0.7324],
          [0.0724, 0.3571, 0.8288]],

         [[0.5698, 0.9723, 0.9213],
          [0.5968, 0.7019, 0.2676],
          [0.9276, 0.6429, 0.1712]]]])
          [0.9276, 0.6429, 0.1712]]]])
>>> a.softmax(dim=0)
tensor([[[[0.2651, 0.4395, 0.4802],
          [0.1324, 0.2154, 0.2099],
          [0.7098, 0.1285, 0.2478]],

         [[0.3669, 0.0951, 0.1450],
          [0.5107, 0.4892, 0.3172],
          [0.2758, 0.2940, 0.3571]]],


        [[[0.7349, 0.5605, 0.5198],
          [0.8676, 0.7846, 0.7901],
          [0.2902, 0.8715, 0.7522]],

         [[0.6331, 0.9049, 0.8550],
          [0.4893, 0.5108, 0.6828],
          [0.7242, 0.7060, 0.6429]]]])
>>>
```

# @
```
>>> a = torch.tensor([[1,2],[3,4]])
>>> a
tensor([[1, 2],
        [3, 4]])
>>> b = torch.tensor([[5,6],[7,8]])
>>> b
tensor([[5, 6],
        [7, 8]])
>>> c = a @ b
>>> c
tensor([[19, 22],
        [43, 50]])
>>> c = b @ a
>>> c
tensor([[23, 34],
        [31, 46]])
>>>
```