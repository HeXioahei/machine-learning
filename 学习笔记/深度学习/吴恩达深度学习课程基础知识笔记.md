# logistic回归
## logistic回归
用于解决二分类问题，使得输出结果在0到1之间，满足概率的形式。就是在线性函数的结果再放入sigmoid函数中进行激活。
$$z=w^{T}x+b$$
$$\hat{y}=a=\sigma(z)$$
$$\sigma(z)=\frac{1}{1+e^{-z}}$$
## logistic回归损失函数
为了训练logistic回归模型的参数w和b，我们需要再定义一个成本函数（代价函数）（cost function）。现在我们来看看用logistic回归来训练的损失函数（误差函数）（loss function）。

最简单一种损失函数L的定义方式是，y_hat 与 y 的差的平方，但是这种定义方式在梯度下降法寻找最小值时效果往往不好，因为它可能存在很多局部最优解（即很多极小值）（为非凸函数）。所以我们要定义一个不一样的损失函数。人们往往是这样定义的：

$$L(y,\hat{y})=-(ylog\hat{y}+(1-y)log(1-\hat{y}))$$

*这里的log是ln的意思*

为什么说这个函数的作用效果很好呢？我们取两个特殊的情况来看看。

如果y=1，我们想让L尽可能小，根据函数，就需要y_hat尽可能大。而y_hat是经过sigmoid处理过后的，所以它不会大过1，它就算再大也只会无限地接近于1，也就是，无限地接近于y，这正是我们想要的效果，y_hat很接近y，预测效果越准确。

如果y=0，我们想让L尽可能小，根据函数，就需要y_hat尽可能小。而y_hat是经过sigmoid处理过后的，所以它不会小过0，它就算再小也只会无限地接近于0，也就是，无限地接近于y，这正是我们想要的效果，y_hat很接近y，预测效果越准确。

并且他是个凸函数，只有一个最优解，只有一个极小值。

所以说，这就很适合二分类问题，因为预测的结果要么为0要么为1，该损失函数会调整参数尽量使得y_hat等于0或1，使得误差最小。

而上面只是定义了作用于单个样本上的损失函数，要考虑整个训练集的话，我们需要定义成本函数，来衡量整个训练集样本的效果。如下：

$$J(w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})$$

展开得：

$$J(w,b)=-\frac{1}{m}\sum_{i=1}^m(y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)}))$$

*上标(i)表示第i个样本点*
# 梯度下降法
我们要寻找合适的w和b，来使J最小。首先要给w和b初始化，无论初始化为多少，最终都可以找到差不多使J最小的w和b，因为logistic回归函数是个凸函数，只有一个极小值。

用`w:`表示更新迭代后的w，梯度下降的计算公式即为：

$$w:=w-\alpha\frac{\partial{J(w,b)}} {\partial{w}}$$
$$b:=b-\alpha\frac{\partial{J(w,b)}} {\partial{b}}$$

# 前向传播和反向传播
## 流程图（单个样本）
先进行前向传播，通过参数w和b计算出y_hat，与y比较，若差距较大，则进行逆向传播，利用梯度下降法更新参数，再继续前向传播，计算出y_hat，与y比较，反复进行，知道差距很小，即J很小为止。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162005466.png)

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162008910.png)

*左边的dz是python代码中的dL/dz，da、dw等也是一样的道理*

**注意**：这里的x1和x2并不是代表两个样本，而是一个样本中的两个特征，也即一个样本中的两个变量。

## 在代码中的实现（伪代码）

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162013477.png)

$dw_{1}$、$dw_{2}$、$db$ 都是累加器，代表这个训练集的，而非单个样本。

但是，这种显示使用for循环来进行梯度下降的方式在python算法种很低效，所以，我们接下来引入向量化技术，它可以加快运算速度。

# 向量化
## 向量化
我们先回顾一下这个公式：

$$z=w^{T}x+b$$

其中w和x都是一个R内的n维列向量，$w\in{R^{n}}$，$x\in{R^{n}}$。

在python的numpy中，可以用`np.dot(w,x)`来计算这两个列向量（也即一维矩阵）的叉乘。完整的为`z=np.dot(w,x)+b`。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162031343.png)

* `u=np.zeros((n,1))`：生成一个全是0的向量。
* `u=np.exp(v)`：将向量v的每个元素都指数化处理。同样的还有`np.log()`、`np.abs()`、`np.maximum(v,0)`（求出v中所有元素和0之间相比的最大值）
* `v**2`：对v中每个元素求平方。`1/v`：每个元素求倒数

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162047347.png)

## 向量化应用在logistic回归中
上面那张图仍然有for循环，可以进一步向量化。如下：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162056795.png)

上图是前向传播的过程，下图是前向传播和逆向传播的全过程。如下：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162113087.png)

上图中，右上角的for循环代表循环进行多次的前向和逆向全过程。

# python中的广播
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162122400.png)

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162122394.png)

为什么要reshape？因为cal的表现形式只是个向量，reshape后才能换成为一个矩阵的表现形式（即有两个中括号），才能和A做运算。不过，即使不显式地调用reshape，其也会默认调用reshape的。axis=0表示竖直方向相加。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162131552.png)

# python中numpy的向量的说明——使用技巧
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162141054.png)

尽量换成矩阵表示法，这样可以避免一些不必要的错误。如下：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162141541.png)

# 神经网络
## 神经网络概述
### 前向传播（蓝色部分）
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162259027.png)

上标[i]表示第i层。

### 逆向传播（红色部分）
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162302036.png)

## 神经网络的表示
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162325905.png)

这个神经网络有三层，但是称为“双层神经网络“，因为输入层被称为第零层，第一个隐藏层才是第一层。隐藏层的每一层和输出层都含有参数w和b。在这个例子中，隐藏层中的w为(4,3)的矩阵、b为(4,1)的矩阵，输出层中的w为(1,4)的矩阵、b为(1,1)的矩阵。

## 神经网络中的计算输出
我们把每一层的单个单元（即图中每个圆圈）放大看一下里面具体装着什么。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162341128.png)

将计算向量化。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162343678.png)

所以此例总结起来就如下四个等式（在python中也就对应着四行代码）：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162352846.png)

