# logistic回归
## logistic回归
用于解决二分类问题，使得输出结果在0到1之间，满足概率的形式。就是在线性函数的结果再放入sigmoid函数中进行激活。
$$z=w^{T}x+b$$
$$\hat{y}=a=\sigma(z)$$
$$\sigma(z)=\frac{1}{1+e^{-z}}$$
## logistic回归损失函数
为了训练logistic回归模型的参数w和b，我们需要再定义一个成本函数（代价函数）（cost function）。现在我们来看看用logistic回归来训练的损失函数（误差函数）（loss function）。

最简单一种损失函数L的定义方式是，y_hat 与 y 的差的平方，但是这种定义方式在梯度下降法寻找最小值时效果往往不好，因为它可能存在很多局部最优解（即很多极小值）（为非凸函数）。所以我们要定义一个不一样的损失函数。人们往往是这样定义的：

$$L(y,\hat{y})=-(ylog\hat{y}+(1-y)log(1-\hat{y}))$$

*这里的log是ln的意思*

为什么说这个函数的作用效果很好呢？我们取两个特殊的情况来看看。

如果y=1，我们想让L尽可能小，根据函数，就需要y_hat尽可能大。而y_hat是经过sigmoid处理过后的，所以它不会大过1，它就算再大也只会无限地接近于1，也就是，无限地接近于y，这正是我们想要的效果，y_hat很接近y，预测效果越准确。

如果y=0，我们想让L尽可能小，根据函数，就需要y_hat尽可能小。而y_hat是经过sigmoid处理过后的，所以它不会小过0，它就算再小也只会无限地接近于0，也就是，无限地接近于y，这正是我们想要的效果，y_hat很接近y，预测效果越准确。

并且他是个凸函数，只有一个最优解，只有一个极小值。

所以说，这就很适合二分类问题，因为预测的结果要么为0要么为1，该损失函数会调整参数尽量使得y_hat等于0或1，使得误差最小。

而上面只是定义了作用于单个样本上的损失函数，要考虑整个训练集的话，我们需要定义成本函数，来衡量整个训练集样本的效果。如下：

$$J(w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})$$

展开得：

$$J(w,b)=-\frac{1}{m}\sum_{i=1}^m(y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)}))$$

*上标(i)表示第i个样本点*
# 梯度下降法
我们要寻找合适的w和b，来使J最小。首先要给w和b初始化，无论初始化为多少，最终都可以找到差不多使J最小的w和b，因为logistic回归函数是个凸函数，只有一个极小值。

用`w:`表示更新迭代后的w，梯度下降的计算公式即为：

$$w:=w-\alpha\frac{\partial{J(w,b)}} {\partial{w}}$$
$$b:=b-\alpha\frac{\partial{J(w,b)}} {\partial{b}}$$

# 前向传播和反向传播
## 流程图（单个样本）
先进行前向传播，通过参数w和b计算出y_hat，与y比较，若差距较大，则进行逆向传播，利用梯度下降法更新参数，再继续前向传播，计算出y_hat，与y比较，反复进行，知道差距很小，即J很小为止。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162005466.png)

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162008910.png)

*左边的dz是python代码中的dL/dz，da、dw等也是一样的道理*

**注意**：这里的x1和x2并不是代表两个样本，而是一个样本中的两个特征，也即一个样本中的两个变量。

## 在代码中的实现（伪代码）

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162013477.png)

$dw_{1}$、$dw_{2}$、$db$ 都是累加器，代表这个训练集的，而非单个样本。

但是，这种显示使用for循环来进行梯度下降的方式在python算法种很低效，所以，我们接下来引入向量化技术，它可以加快运算速度。

# 向量化
## 向量化
我们先回顾一下这个公式：

$$z=w^{T}x+b$$

其中w和x都是一个R内的n维列向量，$w\in{R^{n}}$，$x\in{R^{n}}$。

在python的numpy中，可以用`np.dot(w,x)`来计算这两个列向量（也即一维矩阵）的叉乘。完整的为`z=np.dot(w,x)+b`。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162031343.png)

* `u=np.zeros((n,1))`：生成一个全是0的向量。
* `u=np.exp(v)`：将向量v的每个元素都指数化处理。同样的还有`np.log()`、`np.abs()`、`np.maximum(v,0)`（求出v中所有元素和0之间相比的最大值）
* `v**2`：对v中每个元素求平方。`1/v`：每个元素求倒数

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162047347.png)

## 向量化应用在logistic回归中
上面那张图仍然有for循环，可以进一步向量化。如下：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162056795.png)

上图是前向传播的过程，下图是前向传播和逆向传播的全过程。如下：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162113087.png)

上图中，右上角的for循环代表循环进行多次的前向和逆向全过程。

# python中的广播
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162122400.png)

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162122394.png)

为什么要reshape？因为cal的表现形式只是个向量，reshape后才能换成为一个矩阵的表现形式（即有两个中括号），才能和A做运算。不过，即使不显式地调用reshape，其也会默认调用reshape的。axis=0表示竖直方向相加。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162131552.png)

# python中numpy的向量的说明——使用技巧
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162141054.png)

尽量换成矩阵表示法，这样可以避免一些不必要的错误。如下：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162141541.png)

# 神经网络
## 神经网络概述
### 前向传播（蓝色部分）
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162259027.png)

上标[i]表示第i层。

### 逆向传播（红色部分）
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162302036.png)

## 神经网络的表示
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162325905.png)

这个神经网络有三层，但是称为“双层神经网络“，因为输入层被称为第零层，第一个隐藏层才是第一层。隐藏层的每一层和输出层都含有参数w和b。在这个例子中，隐藏层中的w为(4,3)的矩阵、b为(4,1)的矩阵，输出层中的w为(1,4)的矩阵、b为(1,1)的矩阵。

## 神经网络中的计算输出
我们把每一层的单个单元（即图中每个圆圈）放大看一下里面具体装着什么。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162341128.png)

将计算向量化。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162343678.png)

所以此例总结起来就如下四个等式（在python中也就对应着四行代码）：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410162352846.png)

## 多个样本的向量化
上述的例子只有一个样本，该样本中有三个特征，分别是x1、x2、x3。接下来我们看一下多个样本如何向量化。
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410170008048.png)

如图所示，X的每一列代表一个样本，每一行代表一种特征。Z的每一列代表该层中的每个单元，第一列的第一个就代表第一个单元处理第一个样本后得出的结果，第三列的第二个就代表第三个单元处理第二个样本后得出的结果。A与Z同理。也就是说，对于X，竖着看是不同的特征，横着看是不同样本。对于A和Z，竖着看是不同样本，横着看是不同单元（也就是不同节点）。

## 激活函数
不同层可以有不同的激活函数，用上标`[i]`区分开来。把激活函数表示为$g^{[i]}(z^{[i]})$。神经网络必须有非线性激活函数才能学习。机器学习中的线性回归算法不需要激活函数。

## 随机初始化
若每个参数都初始化为0，同一隐藏层中的每个单元的函数都是一样的，w和b一直是一样的，即使在梯度下降法反向传播时也是一样的。这样，每个单元就具有对称性，设置多个隐藏层单元就没有意义了。

因此，我们要随机初始化参数。对w随机初始化，b可以初始化为0。

一般我们是这样初始化的，
$$w^{[i]} = np.randn((2,2)) * 0.01$$产生参数为(2,2)的高斯分布随机变量，然后再乘以一个很小的数，比如0.01。我们希望权重是很小的一个数，这样z就会比较小，就会集中在激活函数中梯度变化比较明显的部分。
$$b^{[i]}=np.zero((2,1))$$

# 深层神经网络
## 深层神经网络
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410181727358.png)

公式：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410192239817.png)


## 正向与反向传播

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410200808533.png)

cache z，缓存每一层的 z。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410200811406.png)

## 参数和超参数
* 参数：w 和 b
* 超参数：用来控制w和b的参数。如：学习率，梯度下降的次数（即循环次数），隐藏层的个数，每个隐藏层的单元格数，激活函数的选取，最小批次量，正则化参数。

# 进一步了解深层神经网络
## 数据集
* 训练集（train set)：训练模型
* 简单交叉验证集（hold-out cross validation set）/ 验证集（dev set）：选择最好的模型
* 测试集（test set）：对模型进行无偏评估。
对于小数据，百、千、万级别的，train:test=7:3 or train:dev:test=6:2:2。对于大数据，百万级别的，train的比例会大很多，比如，98:1:1。

要确保验证集和测试集的数据来自同一分布。

dev相当于考前的模拟考，test相当于正式考。可以没有test set，这时，就要在dev set上进行评估选出适用的模型。

## 偏差、方差（bias、variance）
如果在train set 中的错误率低，在dev set 中的错误率高，那么可能是训练过拟合了，称为“高方差”。如果两个错误率都挺高的，则说明欠拟合，称为“高偏差”。如果train的错误率挺高，而dev的更高，则既有“高偏差”又有“高方差”。

欠拟合就是训练不够，可能是算法差导致的，也可能是数据量太少了。

将人的肉眼识别误差作为最优误差，也称“基本误差”。

要解决过拟合问题，可以增大数据集或采用正则化。
## 正则化
公式：
$$J(w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||w||^{2}_{2}$$
这个公式是“L2正则化”。$\lambda$即正则化参数，$||w||^2_2$是参数w的L2范数的平方。

后面其实可以再加上$\frac{\lambda}{2m}b^2$，但是也可以不写，因为w是个高纬度参数，已经可以表示高方差问题了。

也可以“L1正则化”。$\frac{\lambda}{2m}||w||_1$。它使得模型变得更稀疏，有一些参数变为了0。我们通常用L2。

*lambda在python中为保留字段，所以，我们在设置变量的时候最好写lambd。*



