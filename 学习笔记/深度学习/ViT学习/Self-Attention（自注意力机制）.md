![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202411141900464.png)

感觉self-attention其实和全连接层差不多，但是self-attention的参数量比全连接层要小。