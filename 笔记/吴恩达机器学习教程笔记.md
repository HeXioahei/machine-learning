# 两种主要类型

1. **Supervised learning 监督学习**：更为广泛使用。

2. **Unsupervised learning 非监督学习**

*还有一种 Reinforcement learning 强化学习，不常用。*

## 监督学习

先用几个示例输入（x）和对应的正确输出（y）来训练模型，然后当我们输入全新的x时，它就会尝试产生适当的相应输出（y）。还有一种类型是分类模型（Classification），其，不像回归那样。

* **回归（Regression）**：predict a number， infinitely many possible outputs 。试图预测无限多个可能数字中的任意一个。比如房价预估模型，找一个合适的拟合曲线来表示房价的变化，之后就可以根据曲线来获取来预测房价。
  
  <img title="" src="file:///C:/Users/Lenovo/Pictures/Typedown/797c0e7e-2bf0-46ac-a272-5c96853ff67c.png" alt="797c0e7e-2bf0-46ac-a272-5c96853ff67c" style="zoom:50%;">

* **分类（Classification）**：predict categories，small number of possible outputs 。对于多个输入只有两种或有限种输出。预测的输出不一定是数字，也可以预测一张图片是猫还是狗。比如预测癌症模型，根据肿瘤大小来预测是良性还是恶性。
  
  <img src="file:///C:/Users/Lenovo/Pictures/Typedown/e22a6a4c-04bb-499e-a953-bae4c36358af.png" title="" alt="e22a6a4c-04bb-499e-a953-bae4c36358af" style="zoom:50%;">

输入参数不一定只有一种，也可以有多种。比如可以用年龄和肿瘤大小两个参数来预测癌症是良性的还是恶性的。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/23500b60-3b6a-4631-8507-d14415f698d7.png" title="" alt="23500b60-3b6a-4631-8507-d14415f698d7" style="zoom:50%;">

而在实际的机器学习中，往往需要大量的输入值来训练。

**总结：监督学习**是 **映射输入x到输出y，learning algorithm 从正确答案中学习。**



## 无监督学习

在监督学习中，每一个示例都与一个输出标签y相关联。而在无监督学习中，给定数据与任何输出标签y无关。比如给定了患者的年龄和肿瘤大小的数据，但不知道肿瘤是良性还是恶性，所以数据集看起来如下：

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/3d02de11-bd30-4289-aca5-8f8fe1352797.png" title="" alt="3d02de11-bd30-4289-aca5-8f8fe1352797" style="zoom:50%;">

我们没有被要求诊断肿瘤是良性的还是恶性的，因为没有给我们任何标签。相反，我们的工作是找到一些结构或模式，或者是在数据集中找一些有趣的东西(Find something interesting in unlabled data)。我们并不想让每个输入都有正确的答案，我们只是想让算法自己找出什么是有趣的，或者什么模式或结构可能在这个数据集中。

* **聚类算法（Clustering algorithm）**：Group similar data points together。无监督学习算法可能决定数据可以分配给两个不同的组或两个不同的集群，这是一种特殊类型的无监督学习，称为聚类算法，它将未标记的数据放入不同的集群中。比如谷歌新闻（Google News）就用了这个，实现了在某条头条新闻的下面跟着很多相关的新闻的功能。还有比如基因或DNA数据聚类（DNA micro array）。再比如很多公司拥有庞大的客户信息数据库，根据这些数据，可以自动地把客户分成不同的类型，更有效更有针对性地提供服务（Grouping customers）。
  ***总结：在没有标签的情况下获取数据，并尝试将它们自动分组到集群中。***

* **异常检测（Anomaly detection algorithm）**：Find unusual data points。用于探测不寻常的事情。这对金融系统中的欺诈检测非常重要。

* **降维（Dimensionality reduction）**：Compress data using fewer numbers。这让我们可以获取一个大数据集，并几乎神奇地将其压缩到一个小得多的数据集，同时损失尽可能少的信息。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/5dc297b5-ca1e-4368-b797-0fabb4d799ac.png" title="" alt="5dc297b5-ca1e-4368-b797-0fabb4d799ac" style="zoom:50%;">

# 工具：Jupyter Notebooks

用来编写代码进行实验和尝试的默认环境。



# 线性回归模型

## 线性回归模型

* Training Set：Data used to train the model。

<img title="" src="file:///C:/Users/Lenovo/Pictures/Typedown/09498916-546b-4608-bd0d-8bedc570d760.png" alt="09498916-546b-4608-bd0d-8bedc570d760" style="zoom:67%;">

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/1a05ab02-3e6e-4671-afb6-d6fd6ea2d96a.png" title="" alt="1a05ab02-3e6e-4671-afb6-d6fd6ea2d96a" style="zoom:67%;">

## 代价函数（Cost Function）

为了实现线性回归，关键的第一步就是定义一个叫做成本函数的东西。成本函数预测“y-hat”，并通过取“y-hat”与“目标y”进行比较，计算二者差的平方。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/2970bcdb-05c5-47ad-be9e-faee9ccbc1fd.png" title="" alt="2970bcdb-05c5-47ad-be9e-faee9ccbc1fd" style="zoom:67%;">

额外的除以2只是为了让我们以后的一些计算更简洁一点。这个叫做 平方误差成本函数（Squared error cost function）。

## 代价函数的直观理解（Cost Function Intuition）

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/c8b47b59-085f-4420-b1a0-867677855605.png" title="" alt="c8b47b59-085f-4420-b1a0-867677855605" style="zoom:67%;">

我们可以简化模型，这样可以方便求w，简化前后w的值是不变的，因为直线的斜率不变。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/4977b9b8-cf20-4f58-93fc-6ed147738531.png" title="" alt="4977b9b8-cf20-4f58-93fc-6ed147738531" style="zoom:67%;">

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/e1bf49cb-58a0-429d-ae49-bdee3ecf227c.png" title="" alt="e1bf49cb-58a0-429d-ae49-bdee3ecf227c" style="zoom:67%;">

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/0f7aa7c5-c9a5-4804-ba37-f3eca5ded36f.png" title="" alt="0f7aa7c5-c9a5-4804-ba37-f3eca5ded36f" style="zoom:67%;">

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/dab2a9ff-1305-426e-9d6e-f40f149eaa19.png" title="" alt="dab2a9ff-1305-426e-9d6e-f40f149eaa19" style="zoom:67%;">

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/91e64a36-5e38-4aa6-a8c5-6e5f7f4e770e.png" title="" alt="91e64a36-5e38-4aa6-a8c5-6e5f7f4e770e" style="zoom:67%;">

线性回归的目的：选择合适的w和b，使得 J 最小。



## 可视化的例子（Visualization examples）

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/404a066f-f882-40a9-8672-4396909f1953.png" title="" alt="404a066f-f882-40a9-8672-4396909f1953" style="zoom:67%;"><img src="file:///C:/Users/Lenovo/Pictures/Typedown/9df676e2-8f16-49a0-9e4c-8df519e3c721.png" title="" alt="9df676e2-8f16-49a0-9e4c-8df519e3c721" style="zoom:67%;">

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/5d6b490c-866c-456a-8916-d0f54983f0a6.png" title="" alt="5d6b490c-866c-456a-8916-d0f54983f0a6" style="zoom:67%;"><img src="file:///C:/Users/Lenovo/Pictures/Typedown/384d70cc-1468-4c2c-a6f5-361448dea3e0.png" title="" alt="384d70cc-1468-4c2c-a6f5-361448dea3e0" style="zoom:67%;">



# 梯度下降（Gradient Descent）

## 梯度下降

除了上图中用一个个 w 和 b 去试出来最合适的最小的 J ，我们有一种更系统的算法来找到 w 和 b 的值，得到最小的 J 。梯度下降在机器学习中无处不在。梯度下降算法适用于一切函数，它用来找到有两个及以上参数的最小代价函数 J 。我们要先对 w 和 b 进行初步猜测，在线性回归中，初始值是多少并不重要，所以我们一般直接将其设为 0 。接下来要做的就是，每次不断地改变一点 w 和 b 的值，直到 J 有希望稳定或接近最小值。有一点值得注意的是，对于某些函数，J 的图像可能不是弓形或锤子形，有可能不只有一个最小值。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/2e4f8b35-45c9-41aa-a679-63c1f3187855.png" title="" alt="2e4f8b35-45c9-41aa-a679-63c1f3187855" style="zoom:67%;">

下面来看一个更复杂的曲面图的例子。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/ca0eed47-a9a5-4f64-9beb-0dfedd415cb6.png" title="" alt="ca0eed47-a9a5-4f64-9beb-0dfedd415cb6" style="zoom:67%;">

这个函数不是平方误差代价函数。对于线性回归，使用方差代价函数最后总得到碗或吊床的形状。而这个代价函数是当我们训练一个**神经网络模型**时可能得到的。

假设我现在站在某个山顶，要以最少的步伐到达某个山谷，我需要先环顾四周，找寻梯度最大最陡的一个方向，迈出一步，然后继续环顾四周，继续寻找最陡的方向，继续迈步，如此般循环下去，直到山谷。这就是通过多个梯度下降来达到目的的步骤。梯度下降有个有趣的性质，我们可以通过选择参数 w 和 b 的起始值来在曲面上选择一个起点，当我们在执行梯度下降时，如果通过调节参数 w 和 b ，使得这一次第一步走的方向和上一次第一步走的方向不一样，后面继续进行梯度下降，循环下去，我们可能会到达另一个山谷。第一个山谷和第二个山谷都称为**局部极小**。

## 实现梯度下降

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/7b5e2b59-3541-42dc-b86e-3fef55a1bd9a.png" title="" alt="7b5e2b59-3541-42dc-b86e-3fef55a1bd9a" style="zoom:67%;">

第一个表达式的意思是，通过获取 w 的当前值并进行少量调整来更新参数 w 。式子中的等号表示赋值；Alpha 称为**学习率（learning rate）**，通常是 0 到 1 之间的一个小正数，他基本上控制了下降时的幅度，所以，如果Alpha的值很大，这相当于一个非常激进的梯度下降过程；最后一项是代价函数关于 w 的偏导，用于决定下降的方向。

第二个表达式的意思和第一个表达式的意思一样。

我们要通过这两个表达式，重复调整参数值，直到算法收敛。算法收敛的意思是，我们达到了一个局部最小值，在这个最小值下，参数 w 和 b 不再随着我们采取的每一个额外步骤而改变很多。

按这个步骤来实现这个梯度下降时，有个细节需要注意，我们是要**同时更新**参数 w 和 b，所以，在写代码的时候，**赋值的顺序**需要特别注意。

## 梯度下降的直观理解

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/773b0434-accf-4ef2-96f2-dcc782547bd1.png" title="" alt="773b0434-accf-4ef2-96f2-dcc782547bd1" style="zoom:67%;">

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/2d80d9b5-4af5-4538-8eaa-1af5a55ec2ab.png" title="" alt="2d80d9b5-4af5-4538-8eaa-1af5a55ec2ab" style="zoom:67%;">

我们以只有单个参数 w 的代价函数 J 为例，可以看到，通过上面表达式的调整，w 的值 正在逐步接近那个合适的使得 J 最小的值。这便是导数项的意义所在。

## 学习率

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/0c0e6499-6715-48a7-84ee-5a8e81a8ba67.png" title="" alt="0c0e6499-6715-48a7-84ee-5a8e81a8ba67" style="zoom:67%;">

若学习率太小，梯度下降的速度就会很慢，需要很久才能到达最低点；若学习率太大，很容易一下子就越过最低点，无法收敛，甚至可能发散。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/684df23e-5681-4ab6-93a8-ccd902621c32.png" title="" alt="684df23e-5681-4ab6-93a8-ccd902621c32" style="zoom:67%;">

可见，如果我们来到了局部最小值点，也即极小值点，那么，导数项就会为零，无论怎么迭代，都无法使 w 的值改变了，都无法跳脱这个山谷了。所以说，即使我们有一个固定的学习率 Alpha，我们也不一定可以继续学习下去，即，不一定可以继续迭代对参数进行改变来达到全局最优解（即最小值）。

步长太小无法跳出当前的局部最小值区域，步长太大易跳过最优解或无法稳定地收敛到最优解。

*我的思考：既然会到达一个局部最小值点使得 w 的值无法再改变，那么是否可以将多个局部最小值拿来比较，得到全局最小值？*

*后面的学习应该会给出答案*

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/bf9fd531-0599-4310-b044-7ebf388dead1.png" title="" alt="bf9fd531-0599-4310-b044-7ebf388dead1" style="zoom:67%;">

当我们接近局部最小值时，梯度下降将自动采取较小的步子，因为当我们接近局部最小值时，导数会自动减小（切线变得平缓），这意味着步子也会自动变小，即使学习率Alpha 保持在某个固定值。（步子是学习率和导数的乘积，即使学习率不变，导数变小，步子也就变小了）

## 线性回归中的梯度下降

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/26fae6f2-7e9e-4d12-9542-f0c6457f3a0c.png" title="" alt="26fae6f2-7e9e-4d12-9542-f0c6457f3a0c" style="zoom:67%;">

左边是线性回归模型，右边是方差代价函数，下面是梯度下降算法。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/e8ec7249-4f45-4f36-9885-10a82121f8aa.png" title="" alt="e8ec7249-4f45-4f36-9885-10a82121f8aa" style="zoom:67%;">

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/4f74d7cc-4875-4859-97a3-5078a071b4d6.png" title="" alt="4f74d7cc-4875-4859-97a3-5078a071b4d6" style="zoom:67%;">

现在让我们来熟悉一下线性回归中的梯度下降是如何工作的。我们在梯度下降中看到一个问题是，它可能结束于局部最小值，而不是结束于全局最小值（如上图）。而在线性回归中，他只有单一的最小值，没有局部最小值（如下图），因为它是碗状（bowl-shape）的。用技术术语来说，这个代价函数是一个凸函数（convex function）。当我们在一个凸函数上实现梯度下降时，一个很好的特点是，只要我们的学习率是适当的，它总是收敛到全局最小值。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/e9574332-a533-4790-a4da-0c2cac6138f1.png" title="" alt="e9574332-a533-4790-a4da-0c2cac6138f1" style="zoom:67%;">

## 运行梯度下降

左上角是模型和数据的图表，右上角是代价函数的等高线图，底下是相同代价函数的表面图。

随着梯度的下降，右上角的点越来越靠近中心，左上角的线也越来越符合数据。

<img src="file:///C:/Users/Lenovo/Pictures/Typedown/8fe7e7a2-cf71-4096-9a96-d5ebbe3555d5.png" title="" alt="8fe7e7a2-cf71-4096-9a96-d5ebbe3555d5" style="zoom:67%;">

这种梯度下降过程称为间歇梯度下降，指的是在梯度下降的每一个步骤上，我们看的是所有的训练数据，而不仅仅是训练数据的子集。


