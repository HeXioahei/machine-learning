# logistic回归
## logistic回归
用于解决二分类问题，使得输出结果在0到1之间，满足概率的形式。就是在线性函数的结果再放入sigmoid函数中进行激活。

## logistic回归损失函数
为了训练logistic回归模型的参数w和b，我们需要再定义一个成本函数（代价函数）（cost function）。现在我们来看看用logistic回归来训练的损失函数（误差函数）（loss function）。

最简单一种损失函数L的定义方式是，y_hat 与 y 的差的平方，但是这种定义方式在梯度下降法寻找最小值时效果往往不好，因为它可能存在很多局部最优解（即很多极小值）（为非凸函数）。所以我们要定义一个不一样的损失函数。人们往往是这样定义的：

$$L(y,\hat{y})=-(ylog\hat{y}+(1-y)log(1-\hat{y}))$$

为什么说这个函数的作用效果很好呢？我们取两个特殊的情况来看看。

如果y=1，我们想让L尽可能小，根据函数，就需要y_hat尽可能大。而y_hat是经过sigmoid处理过后的，所以它不会大过1，它就算再大也只会无限地接近于1，也就是，无限地接近于y，这正是我们想要的效果，y_hat很接近y，预测效果越准确。

如果y=0，我们想让L尽可能小，根据函数，就需要y_hat尽可能小。而y_hat是经过sigmoid处理过后的，所以它不会小过0，它就算再小也只会无限地接近于0，也就是，无限地接近于y，这正是我们想要的效果，y_hat很接近y，预测效果越准确。

并且他是个凸函数，只有一个最优解，只有一个极小值。

所以说，这就很适合二分类问题，因为预测的结果要么为0要么为1，该损失函数会调整参数尽量使得y_hat等于0或1，使得误差最小。

而上面只是定义了作用于单个样本上的损失函数，要考虑整个训练集的话，我们需要定义成本函数，来衡量整个训练集样本的效果。如下：

$$J(w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})$$

展开得：

$$J(w,b)=-\frac{1}{m}\sum_{i=1}^m(y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)}))$$

# 梯度下降法
我们要寻找合适的w和b，来使J最小。首先要给w和b初始化，无论初始化为多少，最终都可以找到差不多使J最小的w和b，因为logistic回归函数是个凸函数，只有一个极小值。

用`w:`表示更新迭代后的w，梯度下降的计算公式即为：

$$w:=w-\alpha\frac{\partial{J(w,b)}} {\partial{w}}$$
$$b:=b-\alpha\frac{\partial{J(w,b)}} {\partial{b}}$$

