{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5d6484-4e6f-4a9c-b160-e8a59d78d87d",
   "metadata": {},
   "source": [
    "### 导入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17f891d-7b56-46b8-ad5c-9f16ca3cc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "original code from rwightman:\n",
    "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\"\"\"\n",
    "\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4f119-dd04-49a1-b102-bb2c06bbf31b",
   "metadata": {},
   "source": [
    "### 定义模型板块、模型类、模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac041b3-c2fd-41da-96bd-7c4d9e982a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "实现随机深度（Stochastic Depth）\n",
    "\n",
    "在ResNet中，每一层的输出都与其输入相加，这样会导致网络的深度不断增加，导致网络的复杂度不断增加。\n",
    "而随机深度（Stochastic Depth）是一种正则化方法，通过随机丢弃网络的某些层来减少网络的深度，从而提升网络的性能。\n",
    "随机深度的实现方法是：在每一层的输出前面加一个丢弃层（DropPath），丢弃层的丢弃率是可学习的，\n",
    "这样就可以在训练过程中，根据丢弃率来随机丢弃网络的某些层，从而达到随机深度的效果。\n",
    "\n",
    "x:输入张量\n",
    "drop_prob:丢弃率，是一个浮点数，即每个路径（或神经元连接）被丢弃的概率。\n",
    "training:是否在训练模式下运行。如果不是，则不进行丢弃，即丢弃率为零。\n",
    "'''\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob  # 计算保留概率\n",
    "\n",
    "    # 生成随机张量\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # 使用diff-dim张量，而不仅仅是2D卷积网络。\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize（二值化）\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "'''\n",
    "将输入的2D图像转化为一个序列的嵌入向量\n",
    "'''\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    # in_c:输入通道数，3，通常为RGB。\n",
    "    # 我在想这里的img_size和patch_size直接以元组的形式传入是否会更方便。\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size) # 将图片大小和patch大小都转换为元组，方便后面的计算\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # 计算patch网格大小\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]  # patchs的数量\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        '''\n",
    "        为何这里的卷积不需要padding呢？\n",
    "        因为stride=kernel_size，每个像素块在一次卷积中都只参与一次。\n",
    "        而padding适用于中间部分像素块由重复参与卷积的情况。\n",
    "        '''\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # 要求输入的图片大小必须和初始化时设置的img_size一致\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        # proj(x)：用二维卷积进行投影，其实就是切割缩小啦。\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "实现多头注意力机制\n",
    "\n",
    "dim:输入token的dim\n",
    "num_heads:多头注意力机制的头数\n",
    "qkv_bias:在查询Q、键K、值V矩阵的线性变换中是否添加偏置\n",
    "qk_scale:用来缩放查询Q、键K矩阵的尺度。默认是None，表示不缩放。\n",
    "attn_drop_ratio:注意力权重矩阵的dropout的概率\n",
    "proj_drop_ratio:投影矩阵的dropout的概率\n",
    "'''\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,   # 输入token的dim\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 proj_drop_ratio=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads  # 每个头的维度\n",
    "        self.scale = qk_scale or head_dim ** -0.5  # 缩放因子，论文中默认为head_dim的倒数。\n",
    "        # 如果参数中没有指定qk_scale，则使用默认的缩放因子。\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) # 生成q、k、v三个矩阵的组合\n",
    "        # 生成q、k、v三个矩阵所用到的权重矩阵是Linear层自己提供的，\n",
    "        # 这权重矩阵的维度是[dim, dim * 3]，在初始化不用满足什么条件（只要不为零就行），因为反正他是个可学习的参数。\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)  # 为什么不用自己定义的DropPath呢？\n",
    "        self.proj = nn.Linear(dim, dim) # 投影的线性层，将注意力输出的维度还原到输入的维度\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio) # 应用在投影输出上的dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "\n",
    "in_features:输入特征的维度\n",
    "hidden_features:隐藏层的维度，默认等于输入特征的维度\n",
    "out_features:输出特征的维度，默认等于输入特征的维度\n",
    "act_layer:激活层，默认是GELU\n",
    "drop:dropout的概率，默认是0.\n",
    "\"\"\"\n",
    "class Mlp(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        # 如果数据小的话，下面这两层可以不要。\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "transformer encoder 中的基本快\n",
    "\n",
    "dim:输入token的dim\n",
    "num_heads:多头注意力机制的头数\n",
    "mlp_ratio:MLP的隐藏层维度与输入token的维度的比值\n",
    "qkv_bias:在查询Q、键K、值V矩阵的线性变换中是否添加偏置\n",
    "qk_scale:用来缩放查询Q、键K矩阵的尺度。默认是None，表示不缩放。\n",
    "drop_ratio:MLP和注意力机制后的dropout的概率\n",
    "attn_drop_ratio:注意力权重矩阵的dropout的概率\n",
    "drop_path_ratio:随机深度丢失的概率，用于正则化。\n",
    "act_layer:激活层\n",
    "norm_layer:归一化层\n",
    "'''\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        # 如果drop_path_ratio=0，则不使用随机深度。如果drop_path_ratio>0，则使用随机深度。 \n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    img_size (int, tuple): input image size\n",
    "    patch_size (int, tuple): patch size\n",
    "    in_c (int): number of input channels\n",
    "    num_classes (int): number of classes for classification head，分类头的输出类别数\n",
    "    embed_dim (int): embedding dimension\n",
    "    depth (int): depth of transformer ，即block的数量\n",
    "    num_heads (int): number of attention heads，多头注意力机制中头的数量\n",
    "    mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "    qkv_bias (bool): enable bias for qkv if True\n",
    "    qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "    representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "    如果有设置，表示层的维度，用于logits层\n",
    "    distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "    是否为蒸馏模型，即是否包含蒸馏token和头（如DeiT模型）\n",
    "    drop_ratio (float): dropout rate\n",
    "    attn_drop_ratio (float): attention dropout rate\n",
    "    drop_path_ratio (float): stochastic depth rate\n",
    "    embed_layer (nn.Module): patch embedding layer\n",
    "    norm_layer: (nn.Module): normalization layer\n",
    "\"\"\"\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,\n",
    "                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 分类token\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None # 蒸馏token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 位置嵌入\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio) # 位置drop层\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule，随机深度衰减规律\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                  norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ]) # 根据depth参数，用列表推导式构建多个block\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.has_logits = True\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                (\"fc\", nn.Linear(embed_dim, representation_size)),\n",
    "                (\"act\", nn.Tanh())\n",
    "            ])) # 用于在分类头之前对特征进行进一步的处理\n",
    "        else:\n",
    "            self.has_logits = False\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:  # 如果distilled=True，则构建蒸馏头\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        # Weight init\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化位置嵌入\n",
    "        if self.dist_token is not None:\n",
    "            nn.init.trunc_normal_(self.dist_token, std=0.02) # 使用截断正态分布初始化蒸馏token\n",
    "\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02) # 使用截断正态分布初始化分类token\n",
    "        self.apply(_init_vit_weights) # 剩下的其他层直接用自定义的权重初始化函数来进行初始化\n",
    "\n",
    "    '''\n",
    "    计算模型的前向传播，但是没有包含最终的分类头。\n",
    "    '''\n",
    "    def forward_features(self, x):\n",
    "        # [B, C, H, W] -> [B, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)  # [B, 196, 768]\n",
    "        # [1, 1, 768] -> [B, 1, 768]\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将类token扩展到batch_size维度上\n",
    "        if self.dist_token is None:  # 如果没有蒸馏token，则直接将cls_token和x拼接起来\n",
    "            x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768]\n",
    "        else: # 如果有蒸馏token，则将cls_token、dist_token和x拼接起来\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "\n",
    "        x = self.pos_drop(x + self.pos_embed) # 加入位置嵌入，然后dropout\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x) \n",
    "        if self.dist_token is None: # 如果没有蒸馏token，则只取分类token的输出\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else: # 如果有蒸馏token，则取分类token和蒸馏token的输出\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    '''\n",
    "    主要的forward函数，包含了前向传播和分类头。\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None: # 如果有蒸馏头，则将分类token和蒸馏token的输出作为输入\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n",
    "            if self.training and not torch.jit.is_scripting(): # 如果是在训练模式而不是在脚本模式下，则返回两个分类的结果\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else: # 否则返回平均值\n",
    "                return (x + x_dist) / 2\n",
    "        else: # 如果没有蒸馏头，则只返回分类token的输出\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "ViT weight initialization 自定义权重初始化函数\n",
    ":param m: module，模型\n",
    "\"\"\"\n",
    "'''\n",
    "截断正态分布是一种正态分布，其概率密度函数在负无穷到正无穷之间，但是在负无穷到正无穷之间只有一小部分区域是有限的，\n",
    "因此，截断正态分布就是将正态分布的概率密度函数在负无穷到正无穷之间截断，使得其概率密度函数在负无穷到正无穷之间变为一个常数，这有助于避免极端值的出现。\n",
    "截断正态分布的标准差可以由参数σ来控制，σ越大，截断的区域越小，分布越集中，σ越小，截断的区域越大，分布越分散。\n",
    "'''\n",
    "def _init_vit_weights(m): \n",
    "    \n",
    "    if isinstance(m, nn.Linear):  # 线性层中的权重初始化\n",
    "        nn.init.trunc_normal_(m.weight, std=.01) # 使用截断正态分布初始化权重，标准差设置为0.01\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d): # 卷积层中的权重初始化\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\") \n",
    "        '''\n",
    "        使用Kaiming初始化权重（也称为He初始化）（因为是何凯明大神提出的），\n",
    "        其考虑了前向传播和反向传播中激活值和梯度值得方差，特别适用于ReLu激活函数。\n",
    "        mode=\"fan_out\"表示根据输出单元得数量来缩放权重\n",
    "        '''\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm): # 层归一化层中的权重初始化\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)\n",
    "\n",
    "'''\n",
    "原论文中提到的ViT-B/16模型，是一个基础模型，其参数量为10M。\n",
    "使用16×16的图像块（patch）大小，并在ImageNet-1k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_base_patch16_224(num_classes: int = 1000):\n",
    "    \"\"\"\n",
    "    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    链接: https://pan.baidu.com/s/1zqb08naP0RPqqfSXfkB2EA  密码: eu9f\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=16,\n",
    "                              embed_dim=768,\n",
    "                              depth=12,\n",
    "                              num_heads=12,\n",
    "                              representation_size=None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中ViT-B/16模型的ImageNet-21k版本，其参数量为11.8M。\n",
    "'''\n",
    "def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=16,\n",
    "                              embed_dim=768,\n",
    "                              depth=12,\n",
    "                              num_heads=12,\n",
    "                              representation_size=768 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中提到的ViT-B/32模型，是一个基础模型，其参数量为15M。\n",
    "使用32×32的图像块（patch）大小，并在ImageNet-1k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_base_patch32_224(num_classes: int = 1000):\n",
    "    \"\"\"\n",
    "    ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    链接: https://pan.baidu.com/s/1hCv0U8pQomwAtHBYc4hmZg  密码: s5hl\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=32,\n",
    "                              embed_dim=768,\n",
    "                              depth=12,\n",
    "                              num_heads=12,\n",
    "                              representation_size=None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中ViT-B/32模型的ImageNet-21k版本，其参数量为17.8M。\n",
    "'''\n",
    "def vit_base_patch32_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch32_224_in21k-8db57226.pth\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=32,\n",
    "                              embed_dim=768,\n",
    "                              depth=12,\n",
    "                              num_heads=12,\n",
    "                              representation_size=768 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中的ViT-L/16模型，是一个大模型，其参数量为30M。\n",
    "使用16×16的图像块（patch）大小，并在ImageNet-1k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_large_patch16_224(num_classes: int = 1000):\n",
    "    \"\"\"\n",
    "    ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    链接: https://pan.baidu.com/s/1cxBgZJJ6qUWPSBNcE4TdRQ  密码: qqt8\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=16,\n",
    "                              embed_dim=1024,\n",
    "                              depth=24,\n",
    "                              num_heads=16,\n",
    "                              representation_size=None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中ViT-L/16模型的ImageNet-21k版本，其参数量为33.8M。\n",
    "'''\n",
    "def vit_large_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch16_224_in21k-606da67d.pth\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=16,\n",
    "                              embed_dim=1024,\n",
    "                              depth=24,\n",
    "                              num_heads=16,\n",
    "                              representation_size=1024 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中提到的ViT-L/32模型，是一个大模型，其参数量为48M。\n",
    "使用32×32的图像块（patch）大小，并在ImageNet-21k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_large_patch32_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch32_224_in21k-9046d2e7.pth\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=32,\n",
    "                              embed_dim=1024,\n",
    "                              depth=24,\n",
    "                              num_heads=16,\n",
    "                              representation_size=1024 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中提到的ViT-H/14模型，是一个超大模型，其参数量为128M。\n",
    "使用14×14的图像块（patch）大小，并在ImageNet-21k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_huge_patch14_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    NOTE: converted weights not currently available, too large for github release hosting.\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=14,\n",
    "                              embed_dim=1280,\n",
    "                              depth=32,\n",
    "                              num_heads=16,\n",
    "                              representation_size=1280 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561d3e9-733c-46e6-b86b-612c3d15f931",
   "metadata": {},
   "source": [
    "### 定义数据集类、数据集加载函数、数据可视化函数等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73df255-825f-4988-beb4-8b24a336599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataSet(Dataset): # 继承Dataset，表明它是一个数据集类型。\n",
    "    \"\"\"自定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, images_path: list, images_class: list, transform=None):\n",
    "        self.images_path = images_path  # 图片路径列表\n",
    "        self.images_class = images_class  # 图片类别列表\n",
    "        self.transform = transform  # （可选）数据预处理方法，比如缩放裁剪等\n",
    "        '''\n",
    "        我看了一下数据集，里面的图片大小不一，而输入网络的图片大小要求是统一为224×224的，\n",
    "        所以肯定要对图片进行预处理，下面进行训练的main函数中就有进行预处理的操作\n",
    "        '''\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    '''\n",
    "    获取第item个样本的图像和标签\n",
    "    '''\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.images_path[item])\n",
    "        # RGB为彩色图片，L为灰度图片\n",
    "        if img.mode != 'RGB':\n",
    "            raise ValueError(\"image: {} isn't RGB mode.\".format(self.images_path[item]))\n",
    "        label = self.images_class[item]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    '''\n",
    "    定义一个静态方法，将多个样本合并为一个批次，处理成适合网络输入的格式\n",
    "\n",
    "    batch: 一个批次的样本列表，格式为[(img1, label1), (img2, label2),..., (imgN, labelN)]\n",
    "    '''\n",
    "    @staticmethod # 静态方法，可以直接通过类名调用\n",
    "    def collate_fn(batch):\n",
    "        # 官方实现的default_collate可以参考\n",
    "        # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py\n",
    "        images, labels = tuple(zip(*batch)) # 解压batch列表，解压得到图片和标签两个列表\n",
    "\n",
    "        images = torch.stack(images, dim=0) # 将图像堆叠为一个张量\n",
    "        labels = torch.as_tensor(labels) # 将标签转换为张量\n",
    "        return images, labels # 返回图像张量和标签张量\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "root: 数据集根目录\n",
    "val_rate: 验证集样本占总样本的比例\n",
    "'''\n",
    "def read_split_data(root: str, val_rate: float = 0.2):\n",
    "    random.seed(0)  # 设置随机种子保证随机结果可复现\n",
    "    assert os.path.exists(root), \"dataset root: {} does not exist.\".format(root) # 确保数据集根目录存在，否则报错\n",
    "\n",
    "    # 遍历文件夹，一个文件夹对应一个类别\n",
    "    flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))] \n",
    "    # 排序，保证各平台顺序一致\n",
    "    flower_class.sort()\n",
    "    # 生成类别名称以及对应的数字索引\n",
    "    class_indices = dict((k, v) for v, k in enumerate(flower_class)) # 创建一个字典，并将名称映射到对应的数字索引\n",
    "    json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4) # 将字典转换为json格式字符串，并缩进为4个空格\n",
    "    with open('class_indices.json', 'w') as json_file: # 保存到class_indices.json文件中\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    train_images_path = []  # 存储训练集的所有图片路径\n",
    "    train_images_label = []  # 存储训练集图片对应索引信息\n",
    "    val_images_path = []  # 存储验证集的所有图片路径\n",
    "    val_images_label = []  # 存储验证集图片对应索引信息\n",
    "    every_class_num = []  # 存储每个类别的样本总数\n",
    "    supported = [\".jpg\", \".JPG\", \".png\", \".PNG\"]  # 支持的文件后缀类型\n",
    "    # 遍历每个文件夹下的文件\n",
    "    for cla in flower_class:\n",
    "        cla_path = os.path.join(root, cla)\n",
    "        # 遍历获取supported支持的所有文件路径\n",
    "        images = [os.path.join(root, cla, i) for i in os.listdir(cla_path)\n",
    "                  if os.path.splitext(i)[-1] in supported]\n",
    "        # 排序，保证各平台顺序一致\n",
    "        images.sort()\n",
    "        # 获取该类别对应的索引，也就是该文件夹下的花图片所对应的类别。\n",
    "        image_class = class_indices[cla]\n",
    "        # 记录该类别的样本数量\n",
    "        every_class_num.append(len(images))\n",
    "        # 按比例随机采样验证样本\n",
    "        val_path = random.sample(images, k=int(len(images) * val_rate))\n",
    "\n",
    "        # 遍历该类别的所有图片，将其分为训练集和验证集\n",
    "        for img_path in images:\n",
    "            if img_path in val_path:  # 如果该路径在采样的验证集样本中则存入验证集\n",
    "                val_images_path.append(img_path)\n",
    "                val_images_label.append(image_class)\n",
    "            else:  # 否则存入训练集\n",
    "                train_images_path.append(img_path)\n",
    "                train_images_label.append(image_class)\n",
    "\n",
    "    print(\"{} images were found in the dataset.\".format(sum(every_class_num)))\n",
    "    print(\"{} images for training.\".format(len(train_images_path)))\n",
    "    print(\"{} images for validation.\".format(len(val_images_path)))\n",
    "    assert len(train_images_path) > 0, \"number of training images must greater than 0.\"\n",
    "    assert len(val_images_path) > 0, \"number of validation images must greater than 0.\"\n",
    "\n",
    "    # 绘制类别分布图（可选）（自己手动修改plot_image的值）\n",
    "    plot_image = False\n",
    "    if plot_image:\n",
    "        # 绘制每种类别个数柱状图\n",
    "        plt.bar(range(len(flower_class)), every_class_num, align='center')\n",
    "        # 将横坐标0,1,2,3,4替换为相应的类别名称\n",
    "        plt.xticks(range(len(flower_class)), flower_class)\n",
    "        # 在柱状图上添加数值标签\n",
    "        for i, v in enumerate(every_class_num):\n",
    "            plt.text(x=i, y=v + 5, s=str(v), ha='center')\n",
    "        # 设置x坐标\n",
    "        plt.xlabel('image class')\n",
    "        # 设置y坐标\n",
    "        plt.ylabel('number of images')\n",
    "        # 设置柱状图的标题\n",
    "        plt.title('flower class distribution')\n",
    "        plt.show()\n",
    "\n",
    "    return train_images_path, train_images_label, val_images_path, val_images_label\n",
    "\n",
    "\n",
    "'''\n",
    "可视化数据加载器中的图像及其对应的标签，就是从数据加载器中获取一个批次的图像并将它们显示出来。\n",
    "\n",
    "data_loader:用于加载数据的迭代器，包含批量的图像和标签。\n",
    "'''\n",
    "def plot_data_loader_image(data_loader):\n",
    "    batch_size = data_loader.batch_size # 当前批次中的样本数量\n",
    "    plot_num = min(batch_size, 4) # 最多可视化4张图片\n",
    "\n",
    "    # 读取class_indices.json文件，获取类别名称对应的数字索引\n",
    "    json_path = './class_indices.json'\n",
    "    assert os.path.exists(json_path), json_path + \" does not exist.\"\n",
    "    json_file = open(json_path, 'r')\n",
    "    class_indices = json.load(json_file)\n",
    "\n",
    "    for data in data_loader:\n",
    "        images, labels = data\n",
    "        for i in range(plot_num):\n",
    "            # [C, H, W] -> [H, W, C]\n",
    "            img = images[i].numpy().transpose(1, 2, 0)\n",
    "            # 反Normalize操作\n",
    "            img = (img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]) * 255\n",
    "            label = labels[i].item()\n",
    "            plt.subplot(1, plot_num, i+1)\n",
    "            plt.xlabel(class_indices[str(label)])\n",
    "            plt.xticks([])  # 去掉x轴的刻度\n",
    "            plt.yticks([])  # 去掉y轴的刻度\n",
    "            plt.imshow(img.astype('uint8'))\n",
    "        plt.show()\n",
    "\n",
    "'''\n",
    "将一个列表对象序列化并保存在一个文件中。\n",
    "\n",
    "list_info: 要序列化的列表对象。\n",
    "file_name: 保存序列化结果的文件名。\n",
    "'''\n",
    "def write_pickle(list_info: list, file_name: str):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(list_info, f)\n",
    "\n",
    "'''\n",
    "读取一个序列化的文件，并返回一个列表对象。\n",
    "\n",
    "file_name: 保存序列化结果的文件名。\n",
    "'''\n",
    "def read_pickle(file_name: str) -> list:\n",
    "    with open(file_name, 'rb') as f:\n",
    "        info_list = pickle.load(f)\n",
    "        return info_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee828e3-89e7-4dcd-90bf-f00b27836d27",
   "metadata": {},
   "source": [
    "### 训练和评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08eb6002-d6a4-4d4a-a2a5-945620c40e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "在一个epoch中训练模型，并返回训练损失和预测正确的样本数。\n",
    "\n",
    "model: 待训练的模型。\n",
    "optimizer: 用于更新模型参数的优化器。\n",
    "data_loader: 用于加载数据的迭代器。\n",
    "device: 运行模型的设备。\n",
    "epoch: 当前epoch轮数。\n",
    "'''\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "\n",
    "    # print(31)\n",
    "    \n",
    "    model.train()  # 开启训练模式\n",
    "    loss_function = torch.nn.CrossEntropyLoss() # 交叉熵损失函数，用于多分类问题\n",
    "    # 初始化两个张量，用于累计损失和正确预测的样本数量\n",
    "    accu_loss = torch.zeros(1).to(device)  \n",
    "    accu_num = torch.zeros(1).to(device)   \n",
    "    optimizer.zero_grad() # 清空之前梯度的累积，准备进行新一次的反向传播。\n",
    "\n",
    "    # print(32)\n",
    "    \n",
    "    sample_num = 0\n",
    "    \n",
    "    data_loader = tqdm(data_loader, file=sys.stdout) # 使用tqdm来包装data_loader，以便在控制台中显示进度条\n",
    "\n",
    "    # print(33)\n",
    "\n",
    "    for step, data in enumerate(data_loader):\n",
    "\n",
    "        # print(34)\n",
    "        \n",
    "        images, labels = data # 读取当前批次的图像和标签\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model(images.to(device)) # 将图像转移到指定的设备上进行计算，输入到模型中，得到模型预测结果\n",
    "        pred_classes = torch.max(pred, dim=1)[1] # 得到模型预测结果中概率最大的类别索引\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum() # 计算并累计预测正确的样本数\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device)) # 计算损失\n",
    "        loss.backward() # 反向传播计算梯度\n",
    "        accu_loss += loss.detach() # 累计损失\n",
    "\n",
    "        # 更新进度条信息，显示当前训练进度\n",
    "        data_loader.desc = \"[train epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n",
    "                                                                               accu_loss.item() / (step + 1),\n",
    "                                                                               # accu_loss.item() / (sample_num),\n",
    "                                                                               accu_num.item() / sample_num)\n",
    "        # 检查损失值是否为有限值，如果不是，则输出警告并退出程序\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss, ending training ', loss)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.step() # 根据计算得到的梯度更新模型参数\n",
    "        optimizer.zero_grad() # 清空梯度，准备进行下一次的反向传播\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num # 返回平均损失和准确率\n",
    "    # return accu_loss.item() / (sample), accu_num.item() / sample_num # 返回平均损失和准确率\n",
    "\n",
    "\n",
    "'''\n",
    "在一个epoch中评估模型，并返回验证损失和预测正确的样本数。\n",
    "和上一个方法几乎是一摸一样的，其实都可以将二者合并了，只要用一个if else判断是训练还是评估就行了。\n",
    "'''\n",
    "@torch.no_grad() # 装饰器，用于禁止计算梯度，节省内存\n",
    "def evaluate(model, data_loader, device, epoch):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval() # 开启评估模式\n",
    "\n",
    "    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "\n",
    "    sample_num = 0\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model(images.to(device))\n",
    "        pred_classes = torch.max(pred, dim=1)[1]\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device))\n",
    "        accu_loss += loss\n",
    "\n",
    "        data_loader.desc = \"[valid epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n",
    "                                                                               accu_loss.item() / (step + 1),\n",
    "                                                                               accu_num.item() / sample_num)\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b448db-3258-4d26-b64f-43bcd6f5ac8a",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc91e2fd-1ec4-4e4e-86b5-376972a97fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670 images were found in the dataset.\n",
      "2939 images for training.\n",
      "731 images for validation.\n",
      "Using 0 dataloader workers every process\n",
      "数据长度： 2939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_7856\\2981271554.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights_dict = torch.load(weights, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "training head.weight\n",
      "training head.bias\n",
      "[train epoch 0] loss: 0.374, acc: 0.945:  60%|██████    | 1771/2939 [01:13<00:48, 24.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 107\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# print(3)\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# 更新学习率\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# print(4)\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# validate\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 36\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch)\u001b[0m\n\u001b[0;32m     33\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m data \u001b[38;5;66;03m# 读取当前批次的图像和标签\u001b[39;00m\n\u001b[0;32m     34\u001b[0m sample_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 36\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 将图像转移到指定的设备上进行计算，输入到模型中，得到模型预测结果\u001b[39;00m\n\u001b[0;32m     37\u001b[0m pred_classes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# 得到模型预测结果中概率最大的类别索引\u001b[39;00m\n\u001b[0;32m     38\u001b[0m accu_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meq(pred_classes, labels\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;66;03m# 计算并累计预测正确的样本数\u001b[39;00m\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 318\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 318\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dist \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# 如果有蒸馏头，则将分类token和蒸馏token的输出作为输入\u001b[39;00m\n\u001b[0;32m    320\u001b[0m         x, x_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dist(x[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[2], line 307\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    304\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_token, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_token\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    306\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed) \u001b[38;5;66;03m# 加入位置嵌入，然后dropout\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x) \n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# 如果没有蒸馏token，则只取分类token的输出\u001b[39;00m\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 209\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    208\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)))\n\u001b[1;32m--> 209\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:202\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\MyProgramFiles\\anaconda3\\envs\\pytorch-GPU\\lib\\site-packages\\torch\\nn\\functional.py:2576\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2574\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2575\u001b[0m     )\n\u001b[1;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_classes=5\n",
    "epochs=10\n",
    "batch_size=1\n",
    "lr=0.001\n",
    "lrf=0.01\n",
    "data_path=\"D:\\\\dataset\\\\flower_photos\" # 换成自己储存数据集的路径\n",
    "model_name='model1'\n",
    "weights=\"D:\\\\dataset\\\\weight\\\\jx_vit_base_patch16_224_in21k-e5005f0a.pth\" # 换成自己储存初始权重集的路径\n",
    "freeze_layers=True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "# 实例化模型\n",
    "model = vit_base_patch16_224_in21k(num_classes=num_classes, has_logits=False).to(device)\n",
    "\n",
    "# 创建权重保存文件夹\n",
    "# if os.path.exists(\"./weights\") is False:\n",
    "#     os.makedirs(\"./weights\")\n",
    "\n",
    "tb_writer = SummaryWriter() # 初始化Tensorboard记录器用于记录训练与验证的损失和准确率，便于后续可视化\n",
    "\n",
    "# 读取数据集\n",
    "train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(data_path)\n",
    "\n",
    "# 数据预处理，比如裁剪，缩放，归一化等\n",
    "data_transform = {\n",
    "    # 训练时\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
    "    # 验证时\n",
    "    \"val\": transforms.Compose([transforms.Resize(256),\n",
    "                               transforms.CenterCrop(224),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])}\n",
    "\n",
    "# 实例化训练数据集\n",
    "train_dataset = MyDataSet(images_path=train_images_path,\n",
    "                          images_class=train_images_label,\n",
    "                          transform=data_transform[\"train\"])\n",
    "\n",
    "# print()\n",
    "# print(train_dataset)\n",
    "# print()\n",
    "\n",
    "# 实例化验证数据集\n",
    "val_dataset = MyDataSet(images_path=val_images_path,\n",
    "                        images_class=val_images_label,\n",
    "                        transform=data_transform[\"val\"])\n",
    "\n",
    "batch_size = batch_size\n",
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "# 实例化数据加载器，支持批处理和多线程处理\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True,\n",
    "                                           num_workers=nw,\n",
    "                                           collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "# print(train_loader)\n",
    "print(\"数据长度：\", len(train_loader.dataset))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         pin_memory=True,\n",
    "                                         num_workers=nw,\n",
    "                                         collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "# print(1)\n",
    "\n",
    "# # 载入预训练权重\n",
    "if weights != \"\":\n",
    "    assert os.path.exists(weights), \"weights file: '{}' not exist.\".format(weights)\n",
    "    weights_dict = torch.load(weights, map_location=device)\n",
    "    # 删除不需要的权重\n",
    "    del_keys = ['head.weight', 'head.bias'] if model.has_logits \\\n",
    "        else ['pre_logits.fc.weight', 'pre_logits.fc.bias', 'head.weight', 'head.bias']\n",
    "    for k in del_keys:\n",
    "        del weights_dict[k]\n",
    "    print(model.load_state_dict(weights_dict, strict=False))\n",
    "\n",
    "# 冻结部分层，以便仅更新特定层的权重\n",
    "if freeze_layers:\n",
    "    for name, para in model.named_parameters():\n",
    "        # 除head, pre_logits外，其他权重全部冻结\n",
    "        if \"head\" not in name and \"pre_logits\" not in name:\n",
    "            para.requires_grad_(False)\n",
    "        else:\n",
    "            print(\"training {}\".format(name))\n",
    "\n",
    "# print(2)\n",
    "\n",
    "#  优化器与学习率调度器，使用SGD（随机梯度下降）和cosine（余弦）学习率\n",
    "pg = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(pg, lr=lr, momentum=0.9, weight_decay=5E-5)\n",
    "# Scheduler https://arxiv.org/pdf/1812.01187.pdf\n",
    "lf = lambda x: ((1 + math.cos(x * math.pi / epochs)) / 2) * (1 - lrf) + lrf  # cosine\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "\n",
    "# print(3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # train\n",
    "    train_loss, train_acc = train_one_epoch(model=model,\n",
    "                                            optimizer=optimizer,\n",
    "                                            data_loader=train_loader,\n",
    "                                            device=device,\n",
    "                                            epoch=epoch)\n",
    "\n",
    "    scheduler.step() # 更新学习率\n",
    "\n",
    "    # print(4)\n",
    "    \n",
    "    # validate\n",
    "    val_loss, val_acc = evaluate(model=model,\n",
    "                                 data_loader=val_loader,\n",
    "                                 device=device,\n",
    "                                 epoch=epoch)\n",
    "\n",
    "    # 记录结果与模型保存\n",
    "    tags = [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"learning_rate\"]\n",
    "    tb_writer.add_scalar(tags[0], train_loss, epoch)\n",
    "    tb_writer.add_scalar(tags[1], train_acc, epoch)\n",
    "    tb_writer.add_scalar(tags[2], val_loss, epoch)\n",
    "    tb_writer.add_scalar(tags[3], val_acc, epoch)\n",
    "    tb_writer.add_scalar(tags[4], optimizer.param_groups[0][\"lr\"], epoch)\n",
    "\n",
    "    # torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch)) \n",
    "    '''\n",
    "    这里是为了把权重保存到一个文件中，这样就随时可以应用于模型进行预测，不用重新训练\n",
    "    但我们每次运行代码，在训练后马上就会用同一个模型进行预测，所以没有必要保存权重\n",
    "    同理，上面的创建weights文件夹也就没有必要了\n",
    "    下面预测部分的重新实例化模型、重新获取权重也就没有必要了，都注释掉\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7770588-2948-4e42-ae6f-ce60b750d7d2",
   "metadata": {},
   "source": [
    "### 预测并显示结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61b181-5cc1-4983-a65a-88f52665108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理模型\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Resize(256),\n",
    "     transforms.CenterCrop(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "# load image\n",
    "img_path = \"D:\\\\dataset\\\\predict\\\\OIP.jpg\" # 换成自己储存待预测图片的路径\n",
    "assert os.path.exists(img_path), \"file: '{}' dose not exist.\".format(img_path)\n",
    "img = Image.open(img_path)\n",
    "plt.imshow(img)\n",
    "# [N, C, H, W]\n",
    "img = data_transform(img) # 数据预处理\n",
    "# expand batch dimension\n",
    "img = torch.unsqueeze(img, dim=0)\n",
    "\n",
    "# read class_indict\n",
    "json_path = './class_indices.json'\n",
    "assert os.path.exists(json_path), \"file: '{}' dose not exist.\".format(json_path)\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    class_indict = json.load(f)\n",
    "\n",
    "'''训练完即预测，所以下面这部分注释掉'''\n",
    "# # create model\n",
    "# model = vit_base_patch16_224_in21k(num_classes=5, has_logits=False).to(device)\n",
    "# # load model weights\n",
    "# model_weight_path = \"./weights/model-9.pth\"\n",
    "# model.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # predict class\n",
    "    output = torch.squeeze(model(img.to(device))).cpu()\n",
    "    predict = torch.softmax(output, dim=0)\n",
    "    predict_cla = torch.argmax(predict).numpy()\n",
    "\n",
    "print_res = \"class: {}   prob: {:.3}\".format(class_indict[str(predict_cla)],\n",
    "                                             predict[predict_cla].numpy())\n",
    "plt.title(print_res)\n",
    "for i in range(len(predict)):\n",
    "    print(\"class: {:10}   prob: {:.3}\".format(class_indict[str(i)],\n",
    "                                              predict[i].numpy()))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3e25f1-c330-4ec0-a5e8-d288ab221ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
