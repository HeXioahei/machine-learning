{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4a4f119-dd04-49a1-b102-bb2c06bbf31b",
   "metadata": {},
   "source": [
    "### 定义可能用到的一系列模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac041b3-c2fd-41da-96bd-7c4d9e982a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "original code from rwightman:\n",
    "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\"\"\"\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "实现随机深度（Stochastic Depth）\n",
    "\n",
    "在ResNet中，每一层的输出都与其输入相加，这样会导致网络的深度不断增加，导致网络的复杂度不断增加。\n",
    "而随机深度（Stochastic Depth）是一种正则化方法，通过随机丢弃网络的某些层来减少网络的深度，从而提升网络的性能。\n",
    "随机深度的实现方法是：在每一层的输出前面加一个丢弃层（DropPath），丢弃层的丢弃率是可学习的，\n",
    "这样就可以在训练过程中，根据丢弃率来随机丢弃网络的某些层，从而达到随机深度的效果。\n",
    "\n",
    "x:输入张量\n",
    "drop_prob:丢弃率，是一个浮点数，即每个路径（或神经元连接）被丢弃的概率。\n",
    "training:是否在训练模式下运行。如果不是，则不进行丢弃，即丢弃率为零。\n",
    "'''\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob  # 计算保留概率\n",
    "\n",
    "    # 生成随机张量\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # 使用diff-dim张量，而不仅仅是2D卷积网络。\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize（二值化）\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "'''\n",
    "将输入的2D图像转化为一个序列的嵌入向量\n",
    "'''\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    # in_c:输入通道数，3，通常为RGB。\n",
    "    # 我在想这里的img_size和patch_size直接以元组的形式传入是否会更方便。\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size) # 将图片大小和patch大小都转换为元组，方便后面的计算\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1]) # 计算patch网格大小\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]  # patchs的数量\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        '''\n",
    "        为何这里的卷积不需要padding呢？\n",
    "        因为stride=kernel_size，每个像素块在一次卷积中都只参与一次。\n",
    "        而padding适用于中间部分像素块由重复参与卷积的情况。\n",
    "        '''\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # 要求输入的图片大小必须和初始化时设置的img_size一致\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        # proj(x)：用二维卷积进行投影，其实就是切割缩小啦。\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "实现多头注意力机制\n",
    "'''\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,   # 输入token的dim\n",
    "                 num_heads=8,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 proj_drop_ratio=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)  # 为什么不用自己定义的DropPath呢？\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "\n",
    "    '''\n",
    "    in_features:输入特征的维度\n",
    "    hidden_features:隐藏层的维度，默认等于输入特征的维度\n",
    "    out_features:输出特征的维度，默认等于输入特征的维度\n",
    "    act_layer:激活层，默认是GELU\n",
    "    drop:dropout的概率，默认是0.\n",
    "    '''\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        # 如果数据小的话，下面这两层可以不要。\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "'''\n",
    "transformer encoder 中的基本快\n",
    "\n",
    "dim:输入token的dim\n",
    "num_heads:多头注意力机制的头数\n",
    "mlp_ratio:MLP的隐藏层维度与输入token的维度的比值\n",
    "qkv_bias:在查询Q、键K、值V矩阵的线性变换中是否添加偏置\n",
    "qk_scale:用来缩放查询Q、键K矩阵的尺度。默认是None，表示不缩放。\n",
    "drop_ratio:MLP和注意力机制后的dropout的概率\n",
    "attn_drop_ratio:注意力权重矩阵的dropout的概率\n",
    "drop_path_ratio:随机深度丢失的概率，用于正则化。\n",
    "act_layer:激活层\n",
    "norm_layer:归一化层\n",
    "'''\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 num_heads,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=False,\n",
    "                 qk_scale=None,\n",
    "                 drop_ratio=0.,\n",
    "                 attn_drop_ratio=0.,\n",
    "                 drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        # 如果drop_path_ratio=0，则不使用随机深度。如果drop_path_ratio>0，则使用随机深度。 \n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, num_classes=1000,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,\n",
    "                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_c (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head，分类头的输出类别数\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer ，即block的数量\n",
    "            num_heads (int): number of attention heads，多头注意力机制中头的数量\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            如果有设置，表示层的维度，用于logits层\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            是否为蒸馏模型，即是否包含蒸馏token和头（如DeiT模型）\n",
    "            drop_ratio (float): dropout rate\n",
    "            attn_drop_ratio (float): attention dropout rate\n",
    "            drop_path_ratio (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # 分类token\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None # 蒸馏token\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim)) # 位置嵌入\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio) # 位置drop层\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule，随机深度衰减规律\n",
    "\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                  norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ]) # 根据depth参数，用列表推导式构建多个block\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.has_logits = True\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                (\"fc\", nn.Linear(embed_dim, representation_size)),\n",
    "                (\"act\", nn.Tanh())\n",
    "            ])) # 用于在分类头之前对特征进行进一步的处理\n",
    "        else:\n",
    "            self.has_logits = False\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:  # 如果distilled=True，则构建蒸馏头\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        # Weight init\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02) # 使用截断正态分布初始化位置嵌入\n",
    "        if self.dist_token is not None:\n",
    "            nn.init.trunc_normal_(self.dist_token, std=0.02) # 使用截断正态分布初始化蒸馏token\n",
    "\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02) # 使用截断正态分布初始化分类token\n",
    "        self.apply(_init_vit_weights) # 剩下的其他层直接用自定义的权重初始化函数来进行初始化\n",
    "\n",
    "    '''\n",
    "    计算模型的前向传播，但是没有包含最终的分类头。\n",
    "    '''\n",
    "    def forward_features(self, x):\n",
    "        # [B, C, H, W] -> [B, num_patches, embed_dim]\n",
    "        x = self.patch_embed(x)  # [B, 196, 768]\n",
    "        # [1, 1, 768] -> [B, 1, 768]\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1) # 将类token扩展到batch_size维度上\n",
    "        if self.dist_token is None:  # 如果没有蒸馏token，则直接将cls_token和x拼接起来\n",
    "            x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768]\n",
    "        else: # 如果有蒸馏token，则将cls_token、dist_token和x拼接起来\n",
    "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "\n",
    "        x = self.pos_drop(x + self.pos_embed) # 加入位置嵌入，然后dropout\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x) \n",
    "        if self.dist_token is None: # 如果没有蒸馏token，则只取分类token的输出\n",
    "            return self.pre_logits(x[:, 0])\n",
    "        else: # 如果有蒸馏token，则取分类token和蒸馏token的输出\n",
    "            return x[:, 0], x[:, 1]\n",
    "\n",
    "    '''\n",
    "    主要的forward函数，包含了前向传播和分类头。\n",
    "    '''\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        if self.head_dist is not None: # 如果有蒸馏头，则将分类token和蒸馏token的输出作为输入\n",
    "            x, x_dist = self.head(x[0]), self.head_dist(x[1])\n",
    "            if self.training and not torch.jit.is_scripting(): # 如果是在训练模式而不是在脚本模式下，则返回两个分类的结果\n",
    "                # during inference, return the average of both classifier predictions\n",
    "                return x, x_dist\n",
    "            else: # 否则返回平均值\n",
    "                return (x + x_dist) / 2\n",
    "        else: # 如果没有蒸馏头，则只返回分类token的输出\n",
    "            x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def _init_vit_weights(m): # 自定义权重初始化函数\n",
    "    \"\"\"\n",
    "    ViT weight initialization\n",
    "    :param m: module，模型\n",
    "    \"\"\"\n",
    "    '''\n",
    "    截断正态分布是一种正态分布，其概率密度函数在负无穷到正无穷之间，但是在负无穷到正无穷之间只有一小部分区域是有限的，\n",
    "    因此，截断正态分布就是将正态分布的概率密度函数在负无穷到正无穷之间截断，使得其概率密度函数在负无穷到正无穷之间变为一个常数，这有助于避免极端值的出现。\n",
    "    截断正态分布的标准差可以由参数σ来控制，σ越大，截断的区域越小，分布越集中，σ越小，截断的区域越大，分布越分散。\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear):  # 线性层中的权重初始化\n",
    "        nn.init.trunc_normal_(m.weight, std=.01) # 使用截断正态分布初始化权重，标准差设置为0.01\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Conv2d): # 卷积层中的权重初始化\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\") \n",
    "        '''\n",
    "        使用Kaiming初始化权重（也称为He初始化）（因为是何凯明大神提出的），\n",
    "        其考虑了前向传播和反向传播中激活值和梯度值得方差，特别适用于ReLu激活函数。\n",
    "        mode=\"fan_out\"表示根据输出单元得数量来缩放权重\n",
    "        '''\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LayerNorm): # 层归一化层中的权重初始化\n",
    "        nn.init.zeros_(m.bias)\n",
    "        nn.init.ones_(m.weight)\n",
    "\n",
    "'''\n",
    "原论文中提到的ViT-B/16模型，是一个基础模型，其参数量为10M。\n",
    "使用16×16的图像块（patch）大小，并在ImageNet-1k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_base_patch16_224(num_classes: int = 1000):\n",
    "    \"\"\"\n",
    "    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    链接: https://pan.baidu.com/s/1zqb08naP0RPqqfSXfkB2EA  密码: eu9f\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=16,\n",
    "                              embed_dim=768,\n",
    "                              depth=12,\n",
    "                              num_heads=12,\n",
    "                              representation_size=None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中ViT-B/16模型的ImageNet-21k版本，其参数量为11.8M。\n",
    "'''\n",
    "def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=16,\n",
    "                              embed_dim=768,\n",
    "                              depth=12,\n",
    "                              num_heads=12,\n",
    "                              representation_size=768 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中提到的ViT-B/32模型，是一个基础模型，其参数量为15M。\n",
    "使用32×32的图像块（patch）大小，并在ImageNet-1k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_base_patch32_224(num_classes: int = 1000):\n",
    "    \"\"\"\n",
    "    ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    链接: https://pan.baidu.com/s/1hCv0U8pQomwAtHBYc4hmZg  密码: s5hl\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=32,\n",
    "                              embed_dim=768,\n",
    "                              depth=12,\n",
    "                              num_heads=12,\n",
    "                              representation_size=None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中ViT-B/32模型的ImageNet-21k版本，其参数量为17.8M。\n",
    "'''\n",
    "def vit_base_patch32_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch32_224_in21k-8db57226.pth\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=32,\n",
    "                              embed_dim=768,\n",
    "                              depth=12,\n",
    "                              num_heads=12,\n",
    "                              representation_size=768 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中的ViT-L/16模型，是一个大模型，其参数量为30M。\n",
    "使用16×16的图像块（patch）大小，并在ImageNet-1k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_large_patch16_224(num_classes: int = 1000):\n",
    "    \"\"\"\n",
    "    ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    链接: https://pan.baidu.com/s/1cxBgZJJ6qUWPSBNcE4TdRQ  密码: qqt8\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=16,\n",
    "                              embed_dim=1024,\n",
    "                              depth=24,\n",
    "                              num_heads=16,\n",
    "                              representation_size=None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中ViT-L/16模型的ImageNet-21k版本，其参数量为33.8M。\n",
    "'''\n",
    "def vit_large_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch16_224_in21k-606da67d.pth\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=16,\n",
    "                              embed_dim=1024,\n",
    "                              depth=24,\n",
    "                              num_heads=16,\n",
    "                              representation_size=1024 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中提到的ViT-L/32模型，是一个大模型，其参数量为48M。\n",
    "使用32×32的图像块（patch）大小，并在ImageNet-21k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_large_patch32_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    weights ported from official Google JAX impl:\n",
    "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch32_224_in21k-9046d2e7.pth\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=32,\n",
    "                              embed_dim=1024,\n",
    "                              depth=24,\n",
    "                              num_heads=16,\n",
    "                              representation_size=1024 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "原论文中提到的ViT-H/14模型，是一个超大模型，其参数量为128M。\n",
    "使用14×14的图像块（patch）大小，并在ImageNet-21k数据集上以224×224的输入尺寸进行预训练。\n",
    "'''\n",
    "def vit_huge_patch14_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
    "    \"\"\"\n",
    "    ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).\n",
    "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
    "    NOTE: converted weights not currently available, too large for github release hosting.\n",
    "    \"\"\"\n",
    "    model = VisionTransformer(img_size=224,\n",
    "                              patch_size=14,\n",
    "                              embed_dim=1280,\n",
    "                              depth=32,\n",
    "                              num_heads=16,\n",
    "                              representation_size=1280 if has_logits else None,\n",
    "                              num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561d3e9-733c-46e6-b86b-612c3d15f931",
   "metadata": {},
   "source": [
    "### 定义一系列工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e73df-c833-4eb1-98c9-a01b75a9d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "root: 数据集根目录\n",
    "val_rate: 验证集样本占总样本的比例\n",
    "'''\n",
    "def read_split_data(root: str, val_rate: float = 0.2):\n",
    "    random.seed(0)  # 设置随机种子保证随机结果可复现\n",
    "    assert os.path.exists(root), \"dataset root: {} does not exist.\".format(root) # 确保数据集根目录存在，否则报错\n",
    "\n",
    "    # 遍历文件夹，一个文件夹对应一个类别\n",
    "    flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))] \n",
    "    # 排序，保证各平台顺序一致\n",
    "    flower_class.sort()\n",
    "    # 生成类别名称以及对应的数字索引\n",
    "    class_indices = dict((k, v) for v, k in enumerate(flower_class)) # 创建一个字典，并将名称映射到对应的数字索引\n",
    "    json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4) # 将字典转换为json格式字符串，并缩进为4个空格\n",
    "    with open('class_indices.json', 'w') as json_file: # 保存到class_indices.json文件中\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    train_images_path = []  # 存储训练集的所有图片路径\n",
    "    train_images_label = []  # 存储训练集图片对应索引信息\n",
    "    val_images_path = []  # 存储验证集的所有图片路径\n",
    "    val_images_label = []  # 存储验证集图片对应索引信息\n",
    "    every_class_num = []  # 存储每个类别的样本总数\n",
    "    supported = [\".jpg\", \".JPG\", \".png\", \".PNG\"]  # 支持的文件后缀类型\n",
    "    # 遍历每个文件夹下的文件\n",
    "    for cla in flower_class:\n",
    "        cla_path = os.path.join(root, cla)\n",
    "        # 遍历获取supported支持的所有文件路径\n",
    "        images = [os.path.join(root, cla, i) for i in os.listdir(cla_path)\n",
    "                  if os.path.splitext(i)[-1] in supported]\n",
    "        # 排序，保证各平台顺序一致\n",
    "        images.sort()\n",
    "        # 获取该类别对应的索引，也就是该文件夹下的花图片所对应的类别。\n",
    "        image_class = class_indices[cla]\n",
    "        # 记录该类别的样本数量\n",
    "        every_class_num.append(len(images))\n",
    "        # 按比例随机采样验证样本\n",
    "        val_path = random.sample(images, k=int(len(images) * val_rate))\n",
    "\n",
    "        # 遍历该类别的所有图片，将其分为训练集和验证集\n",
    "        for img_path in images:\n",
    "            if img_path in val_path:  # 如果该路径在采样的验证集样本中则存入验证集\n",
    "                val_images_path.append(img_path)\n",
    "                val_images_label.append(image_class)\n",
    "            else:  # 否则存入训练集\n",
    "                train_images_path.append(img_path)\n",
    "                train_images_label.append(image_class)\n",
    "\n",
    "    print(\"{} images were found in the dataset.\".format(sum(every_class_num)))\n",
    "    print(\"{} images for training.\".format(len(train_images_path)))\n",
    "    print(\"{} images for validation.\".format(len(val_images_path)))\n",
    "    assert len(train_images_path) > 0, \"number of training images must greater than 0.\"\n",
    "    assert len(val_images_path) > 0, \"number of validation images must greater than 0.\"\n",
    "\n",
    "    # 绘制类别分布图（可选）（自己手动修改plot_image的值）\n",
    "    plot_image = False\n",
    "    if plot_image:\n",
    "        # 绘制每种类别个数柱状图\n",
    "        plt.bar(range(len(flower_class)), every_class_num, align='center')\n",
    "        # 将横坐标0,1,2,3,4替换为相应的类别名称\n",
    "        plt.xticks(range(len(flower_class)), flower_class)\n",
    "        # 在柱状图上添加数值标签\n",
    "        for i, v in enumerate(every_class_num):\n",
    "            plt.text(x=i, y=v + 5, s=str(v), ha='center')\n",
    "        # 设置x坐标\n",
    "        plt.xlabel('image class')\n",
    "        # 设置y坐标\n",
    "        plt.ylabel('number of images')\n",
    "        # 设置柱状图的标题\n",
    "        plt.title('flower class distribution')\n",
    "        plt.show()\n",
    "\n",
    "    return train_images_path, train_images_label, val_images_path, val_images_label\n",
    "\n",
    "'''\n",
    "可视化数据加载器中的图像及其对应的标签，就是从数据加载器中获取一个批次的图像并将它们显示出来。\n",
    "\n",
    "data_loader:用于加载数据的迭代器，包含批量的图像和标签。\n",
    "'''\n",
    "def plot_data_loader_image(data_loader):\n",
    "    batch_size = data_loader.batch_size # 当前批次中的样本数量\n",
    "    plot_num = min(batch_size, 4) # 最多可视化4张图片\n",
    "\n",
    "    # 读取class_indices.json文件，获取类别名称对应的数字索引\n",
    "    json_path = './class_indices.json'\n",
    "    assert os.path.exists(json_path), json_path + \" does not exist.\"\n",
    "    json_file = open(json_path, 'r')\n",
    "    class_indices = json.load(json_file)\n",
    "\n",
    "    for data in data_loader:\n",
    "        images, labels = data\n",
    "        for i in range(plot_num):\n",
    "            # [C, H, W] -> [H, W, C]\n",
    "            img = images[i].numpy().transpose(1, 2, 0)\n",
    "            # 反Normalize操作\n",
    "            img = (img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]) * 255\n",
    "            label = labels[i].item()\n",
    "            plt.subplot(1, plot_num, i+1)\n",
    "            plt.xlabel(class_indices[str(label)])\n",
    "            plt.xticks([])  # 去掉x轴的刻度\n",
    "            plt.yticks([])  # 去掉y轴的刻度\n",
    "            plt.imshow(img.astype('uint8'))\n",
    "        plt.show()\n",
    "\n",
    "'''\n",
    "将一个列表对象序列化并保存在一个文件中。\n",
    "\n",
    "list_info: 要序列化的列表对象。\n",
    "file_name: 保存序列化结果的文件名。\n",
    "'''\n",
    "def write_pickle(list_info: list, file_name: str):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(list_info, f)\n",
    "\n",
    "'''\n",
    "读取一个序列化的文件，并返回一个列表对象。\n",
    "\n",
    "file_name: 保存序列化结果的文件名。\n",
    "'''\n",
    "def read_pickle(file_name: str) -> list:\n",
    "    with open(file_name, 'rb') as f:\n",
    "        info_list = pickle.load(f)\n",
    "        return info_list\n",
    "\n",
    "'''\n",
    "在一个epoch中训练模型，并返回训练损失和预测正确的样本数。\n",
    "\n",
    "model: 待训练的模型。\n",
    "optimizer: 用于更新模型参数的优化器。\n",
    "data_loader: 用于加载数据的迭代器。\n",
    "device: 运行模型的设备。\n",
    "epoch: 当前epoch轮数。\n",
    "'''\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()  # 开启训练模式\n",
    "    loss_function = torch.nn.CrossEntropyLoss() # 交叉熵损失函数，用于多分类问题\n",
    "    # 初始化两个张量，用于累计损失和正确预测的样本数量\n",
    "    accu_loss = torch.zeros(1).to(device)  \n",
    "    accu_num = torch.zeros(1).to(device)   \n",
    "    optimizer.zero_grad() # 清空之前梯度的累积，准备进行新一次的反向传播。\n",
    "\n",
    "    sample_num = 0\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout) # 使用tqdm来包装data_loader，以便在控制台中显示进度条\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data # 读取当前批次的图像和标签\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model(images.to(device)) # 将图像转移到指定的设备上进行计算，输入到模型中，得到模型预测结果\n",
    "        pred_classes = torch.max(pred, dim=1)[1] # 得到模型预测结果中概率最大的类别索引\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum() # 计算并累计预测正确的样本数\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device)) # 计算损失\n",
    "        loss.backward() # 反向传播计算梯度\n",
    "        accu_loss += loss.detach() # 累计损失\n",
    "\n",
    "        # 更新进度条信息，显示当前训练进度\n",
    "        data_loader.desc = \"[train epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n",
    "                                                                               accu_loss.item() / (step + 1),\n",
    "                                                                               accu_num.item() / sample_num)\n",
    "        # 检查损失值是否为有限值，如果不是，则输出警告并退出程序\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss, ending training ', loss)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.step() # 根据计算得到的梯度更新模型参数\n",
    "        optimizer.zero_grad() # 清空梯度，准备进行下一次的反向传播\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num # 返回平均损失和准确率\n",
    "\n",
    "'''\n",
    "在一个epoch中评估模型，并返回验证损失和预测正确的样本数。\n",
    "和上一个方法几乎是一摸一样的，其实都可以将二者合并了，只要用一个if else判断是训练还是评估就行了。\n",
    "'''\n",
    "@torch.no_grad() # 装饰器，用于禁止计算梯度，节省内存\n",
    "def evaluate(model, data_loader, device, epoch):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval() # 开启评估模式\n",
    "\n",
    "    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "\n",
    "    sample_num = 0\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model(images.to(device))\n",
    "        pred_classes = torch.max(pred, dim=1)[1]\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device))\n",
    "        accu_loss += loss\n",
    "\n",
    "        data_loader.desc = \"[valid epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n",
    "                                                                               accu_loss.item() / (step + 1),\n",
    "                                                                               accu_num.item() / sample_num)\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7266ba-9a17-4a48-a39b-f23e15e72cb5",
   "metadata": {},
   "source": [
    "### 定义加载数据集的相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73df255-825f-4988-beb4-8b24a336599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataSet(Dataset): # 继承Dataset，表明它是一个数据集类型。\n",
    "    \"\"\"自定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, images_path: list, images_class: list, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.images_class = images_class\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.images_path[item])\n",
    "        # RGB为彩色图片，L为灰度图片\n",
    "        if img.mode != 'RGB':\n",
    "            raise ValueError(\"image: {} isn't RGB mode.\".format(self.images_path[item]))\n",
    "        label = self.images_class[item]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # 官方实现的default_collate可以参考\n",
    "        # https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py\n",
    "        images, labels = tuple(zip(*batch))\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "        labels = torch.as_tensor(labels)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b448db-3258-4d26-b64f-43bcd6f5ac8a",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91e2fd-1ec4-4e4e-86b5-376972a97fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670 images were found in the dataset.\n",
      "2939 images for training.\n",
      "731 images for validation.\n",
      "Using 8 dataloader workers every process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_23692\\2686108832.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights_dict = torch.load(args.weights, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n",
      "training head.weight\n",
      "training head.bias\n",
      "  0%|          | 0/368 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# from my_dataset import MyDataSet\n",
    "# from vit_model import vit_base_patch16_224_in21k as create_model\n",
    "# from utils import read_split_data, train_one_epoch, evaluate\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    device = torch.device(args.device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if os.path.exists(\"./weights\") is False:\n",
    "        os.makedirs(\"./weights\")\n",
    "\n",
    "    tb_writer = SummaryWriter()\n",
    "\n",
    "    train_images_path, train_images_label, val_images_path, val_images_label = read_split_data(args.data_path)\n",
    "\n",
    "    data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
    "        \"val\": transforms.Compose([transforms.Resize(256),\n",
    "                                   transforms.CenterCrop(224),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])}\n",
    "\n",
    "    # 实例化训练数据集\n",
    "    train_dataset = MyDataSet(images_path=train_images_path,\n",
    "                              images_class=train_images_label,\n",
    "                              transform=data_transform[\"train\"])\n",
    "\n",
    "    # 实例化验证数据集\n",
    "    val_dataset = MyDataSet(images_path=val_images_path,\n",
    "                            images_class=val_images_label,\n",
    "                            transform=data_transform[\"val\"])\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "    print('Using {} dataloader workers every process'.format(nw))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               pin_memory=True,\n",
    "                                               num_workers=nw,\n",
    "                                               collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             pin_memory=True,\n",
    "                                             num_workers=nw,\n",
    "                                             collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "    # model = create_model(num_classes=args.num_classes, has_logits=False).to(device)\n",
    "    model = vit_base_patch16_224_in21k(num_classes=args.num_classes, has_logits=False).to(device)\n",
    "\n",
    "    if args.weights != \"\":\n",
    "        assert os.path.exists(args.weights), \"weights file: '{}' not exist.\".format(args.weights)\n",
    "        weights_dict = torch.load(args.weights, map_location=device)\n",
    "        # 删除不需要的权重\n",
    "        del_keys = ['head.weight', 'head.bias'] if model.has_logits \\\n",
    "            else ['pre_logits.fc.weight', 'pre_logits.fc.bias', 'head.weight', 'head.bias']\n",
    "        for k in del_keys:\n",
    "            del weights_dict[k]\n",
    "        print(model.load_state_dict(weights_dict, strict=False))\n",
    "\n",
    "    if args.freeze_layers:\n",
    "        for name, para in model.named_parameters():\n",
    "            # 除head, pre_logits外，其他权重全部冻结\n",
    "            if \"head\" not in name and \"pre_logits\" not in name:\n",
    "                para.requires_grad_(False)\n",
    "            else:\n",
    "                print(\"training {}\".format(name))\n",
    "\n",
    "    pg = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.SGD(pg, lr=args.lr, momentum=0.9, weight_decay=5E-5)\n",
    "    # Scheduler https://arxiv.org/pdf/1812.01187.pdf\n",
    "    lf = lambda x: ((1 + math.cos(x * math.pi / args.epochs)) / 2) * (1 - args.lrf) + args.lrf  # cosine\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        # train\n",
    "        train_loss, train_acc = train_one_epoch(model=model,\n",
    "                                                optimizer=optimizer,\n",
    "                                                data_loader=train_loader,\n",
    "                                                device=device,\n",
    "                                                epoch=epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # validate\n",
    "        val_loss, val_acc = evaluate(model=model,\n",
    "                                     data_loader=val_loader,\n",
    "                                     device=device,\n",
    "                                     epoch=epoch)\n",
    "\n",
    "        tags = [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"learning_rate\"]\n",
    "        tb_writer.add_scalar(tags[0], train_loss, epoch)\n",
    "        tb_writer.add_scalar(tags[1], train_acc, epoch)\n",
    "        tb_writer.add_scalar(tags[2], val_loss, epoch)\n",
    "        tb_writer.add_scalar(tags[3], val_acc, epoch)\n",
    "        tb_writer.add_scalar(tags[4], optimizer.param_groups[0][\"lr\"], epoch)\n",
    "\n",
    "        torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--num_classes', type=int, default=5)\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=8)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--lrf', type=float, default=0.01)\n",
    "    # parser.add_argument('5', type=int, default=5)\n",
    "    # parser.add_argument('10', type=int, default=10)\n",
    "    # parser.add_argument('8', type=int, default=8)\n",
    "    # parser.add_argument('0.001', type=float, default=0.001)\n",
    "    # parser.add_argument('0.01', type=float, default=0.01)\n",
    "\n",
    "    # 数据集所在根目录\n",
    "    # https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
    "    # parser.add_argument('--data-path', type=str,\n",
    "    #                     default=\"/data/flower_photos\")\n",
    "    parser.add_argument('--data-path', type=str,\n",
    "                        default='D:\\dataset\\flower_photos')\n",
    "    parser.add_argument('--model-name', default='model1', help='create model name')\n",
    "\n",
    "    # 预训练权重路径，如果不想载入就设置为空字符\n",
    "    # parser.add_argument('--weights', type=str, default='./vit_base_patch16_224_in21k.pth',\n",
    "    #                     help='initial weights path')\n",
    "    parser.add_argument('--weights', type=str, default='D:\\dataset\\weight\\jx_vit_base_patch16_224_in21k-e5005f0a.pth',\n",
    "                        help='initial weights path')\n",
    "    # 是否冻结权重\n",
    "    parser.add_argument('--freeze-layers', type=bool, default=True)\n",
    "    parser.add_argument('--device', default='cuda:0', help='device id (i.e. 0 or 0,1 or cpu)')\n",
    "    \n",
    "\n",
    "    # opt = parser.parse_args()\n",
    "    opt = parser.parse_args(args=[\n",
    "         '--num_classes','5',\n",
    "         '--epochs','10',\n",
    "         '--batch-size','8',\n",
    "         '--lr','0.001',\n",
    "         '--lrf','0.01',\n",
    "         '--data-path','D:\\\\dataset\\\\flower_photos',\n",
    "         '--model-name','model1',\n",
    "         '--weights','D:\\\\dataset\\\\weight\\\\jx_vit_base_patch16_224_in21k-e5005f0a.pth',\n",
    "         '--freeze-layers','True',\n",
    "         '--device','cuda:0'\n",
    "     ])\n",
    "\n",
    "    main(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7770588-2948-4e42-ae6f-ce60b750d7d2",
   "metadata": {},
   "source": [
    "### 预测并显示结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61b181-5cc1-4983-a65a-88f52665108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vit_model import vit_base_patch16_224_in21k as create_model\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    data_transform = transforms.Compose(\n",
    "        [transforms.Resize(256),\n",
    "         transforms.CenterCrop(224),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "    # load image\n",
    "    # img_path = \"../tulip.jpg\"\n",
    "    img_path = \"D:\\dataset\\predict\\OIP.jpg\n",
    "    assert os.path.exists(img_path), \"file: '{}' dose not exist.\".format(img_path)\n",
    "    img = Image.open(img_path)\n",
    "    plt.imshow(img)\n",
    "    # [N, C, H, W]\n",
    "    img = data_transform(img)\n",
    "    # expand batch dimension\n",
    "    img = torch.unsqueeze(img, dim=0)\n",
    "\n",
    "    # read class_indict\n",
    "    json_path = './class_indices.json'\n",
    "    assert os.path.exists(json_path), \"file: '{}' dose not exist.\".format(json_path)\n",
    "\n",
    "    with open(json_path, \"r\") as f:\n",
    "        class_indict = json.load(f)\n",
    "\n",
    "    # create model\n",
    "    model = create_model(num_classes=5, has_logits=False).to(device)\n",
    "    # load model weights\n",
    "    # model_weight_path = \"./weights/model-9.pth\"\n",
    "    model_weight_path = \"D:\\dataset\\weight\\jx_vit_base_patch16_224_in21k-e5005f0a.pth\"\n",
    "    model.load_state_dict(torch.load(model_weight_path, map_location=device))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # predict class\n",
    "        output = torch.squeeze(model(img.to(device))).cpu()\n",
    "        predict = torch.softmax(output, dim=0)\n",
    "        predict_cla = torch.argmax(predict).numpy()\n",
    "\n",
    "    print_res = \"class: {}   prob: {:.3}\".format(class_indict[str(predict_cla)],\n",
    "                                                 predict[predict_cla].numpy())\n",
    "    plt.title(print_res)\n",
    "    for i in range(len(predict)):\n",
    "        print(\"class: {:10}   prob: {:.3}\".format(class_indict[str(i)],\n",
    "                                                  predict[i].numpy()))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
