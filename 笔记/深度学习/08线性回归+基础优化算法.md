# 损失函数
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410062348123.png)

找到合适的参数w和b，使得损失最小（即残差最小）
# 求解
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410062350857.png)
只有线性回归具有显示解，即可以用表达式表达的解。线性回归是单层神经网络。

# 梯度下降
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410062355705.png)

# 选择学习率
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410062356951.png)

# 小批量随机梯度下降
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410062357146.png)
批量太小的话不适合并行来最大利用计算资源，太大的话内存消耗过大浪费计算资源（比如所有的样本都是相同的时候）

# 总结
* 梯度下降就是通过不断沿着反梯度方向更新参数求解
* 小批量随机梯度下降是深度学习默认的求解算法
* 两个重要的超参数是批量大小和学习率