{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65c1b98-54f7-4cce-bae5-266fff440b40",
   "metadata": {},
   "source": [
    "### 多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d82eca-68a6-4133-84b8-b7f3601e7a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n",
      "tensor([[0.0425, 0.3996, 0.0564, 0.7655, 0.1120, 0.1696, 0.5256, 0.5552, 0.5087,\n",
      "         0.8599, 0.6043, 0.1248, 0.8323, 0.0841, 0.4176, 0.6518, 0.7897, 0.5848,\n",
      "         0.1280, 0.0270],\n",
      "        [0.4337, 0.6388, 0.4869, 0.7589, 0.4771, 0.0353, 0.7225, 0.6173, 0.4926,\n",
      "         0.6505, 0.9257, 0.8357, 0.6483, 0.4285, 0.7880, 0.2571, 0.4744, 0.7820,\n",
      "         0.7323, 0.2980]])\n",
      "tensor([[ 0.2059,  0.0981, -0.2152,  0.2446, -0.0161, -0.0482, -0.1010, -0.2831,\n",
      "          0.0262, -0.0369],\n",
      "        [ 0.2450,  0.1175, -0.1669,  0.1156, -0.0504,  0.1044,  0.0087, -0.2527,\n",
      "          0.2466, -0.0079]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))  # 定义了一个特殊的 Module（块）\n",
    "# 这是一个简单的单层神经网络，\n",
    "# 一个线性层（20是输入维度，256是隐藏层的神经元个数）\n",
    "# + 一个ReLU（激活函数）\n",
    "# + 一个线性层（256是隐藏层的神经元个数，10s是输出维度）\n",
    "# nn.Linear()会自动初始化权重\n",
    "\n",
    "X = torch.rand(2,20) # 生成一个随机的input  \n",
    "# 2是pn大小（批量大小），20是维度，也可以看作是2*20的矩阵\n",
    "\n",
    "X2 = net(X)  # 将input放入模型中得到output\n",
    "\n",
    "print(345)\n",
    "print(X)\n",
    "print(X2)  # 输出经过模型处理后的output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572c074-2085-42b3-a5b2-d9f241abe432",
   "metadata": {},
   "source": [
    "### 自定义块\n",
    "\n",
    "任何一个 层 或者 神经网络 都是 Module 的一个子类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42751ea3-beab-47ab-9a8b-5600fe1e36cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):   # MLP继承自 Module\n",
    "    # 一共有两个重要的函数\n",
    "    # 一个是__init__函数，在里面定义我们需要用到的属性\n",
    "    def __init__(self):    \n",
    "        super().__init__()  # 继承父类的__init__，它会帮我们把一些参数都初始化好\n",
    "        # 两个全连接层\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)    # 输出层\n",
    "        \n",
    "    # 前向传播函数\n",
    "    def forward(self, X):   # X 输入的数据\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "        # 先把input放到隐藏层里，这里的 hidden() 是继承自父类的一个动态方法，而不是我们定义的 hidden 属性\n",
    "        # 然后调用 functional 的 relu函数（即激活函数）来激活\n",
    "        # 然后放到输出函数里放回输出\n",
    "        # 这个 out() 是继承自父类的一个动态方法\n",
    "        \n",
    "    # 上一个cell的nn.ReLU()是创建一个ReLU类型的对象并作为参数传入到nn.Sequential中进行使用，\n",
    "    # 而这里的F.relu()是直接调用激活函数\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414cd2f-6ff5-4c7a-a92b-2ef02cf5155d",
   "metadata": {},
   "source": [
    "### 实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b55b04-cd5d-4185-a437-65555d44ee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0425, 0.3996, 0.0564, 0.7655, 0.1120, 0.1696, 0.5256, 0.5552, 0.5087,\n",
      "         0.8599, 0.6043, 0.1248, 0.8323, 0.0841, 0.4176, 0.6518, 0.7897, 0.5848,\n",
      "         0.1280, 0.0270],\n",
      "        [0.4337, 0.6388, 0.4869, 0.7589, 0.4771, 0.0353, 0.7225, 0.6173, 0.4926,\n",
      "         0.6505, 0.9257, 0.8357, 0.6483, 0.4285, 0.7880, 0.2571, 0.4744, 0.7820,\n",
      "         0.7323, 0.2980]])\n",
      "tensor([[ 0.0847, -0.0005, -0.0721, -0.0458,  0.0151,  0.0037, -0.0317,  0.0987,\n",
      "          0.0328,  0.0817],\n",
      "        [ 0.1048,  0.1360, -0.1037, -0.0899,  0.0462,  0.1015, -0.1689,  0.1203,\n",
      "          0.1553,  0.1069]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net2 = MLP()  \n",
    "X2 = net2(X)\n",
    "# 这里实例化MLP类后，net2()会自动调用继承自Module类的魔法方法__call__()，进而调用forward()方法\n",
    "print(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc707221-abb6-4064-b734-b20baab70227",
   "metadata": {},
   "source": [
    "### 顺序快\n",
    "模拟实现nn.Sequential()这个类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd3909b-72c6-44ec-94f6-9a51b60117c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0425, 0.3996, 0.0564, 0.7655, 0.1120, 0.1696, 0.5256, 0.5552, 0.5087,\n",
      "         0.8599, 0.6043, 0.1248, 0.8323, 0.0841, 0.4176, 0.6518, 0.7897, 0.5848,\n",
      "         0.1280, 0.0270],\n",
      "        [0.4337, 0.6388, 0.4869, 0.7589, 0.4771, 0.0353, 0.7225, 0.6173, 0.4926,\n",
      "         0.6505, 0.9257, 0.8357, 0.6483, 0.4285, 0.7880, 0.2571, 0.4744, 0.7820,\n",
      "         0.7323, 0.2980]])\n",
      "tensor([[ 0.0847, -0.0005, -0.0721, -0.0458,  0.0151,  0.0037, -0.0317,  0.0987,\n",
      "          0.0328,  0.0817],\n",
      "        [ 0.1048,  0.1360, -0.1037, -0.0899,  0.0462,  0.1015, -0.1689,  0.1203,\n",
      "          0.1553,  0.1069]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):  \n",
    "        # '*args'：list of inpt arguments，即一个收集参数的迭代器，\n",
    "        # 相当于把若干个参数打包成一个来传入，然后依次被调用，是以字典的形式打包的\n",
    "        super().__init__()\n",
    "        for block in args:  \n",
    "            # 依次调用传入的一个个参数，通过第一个cell里的代码可以知道，也即一个个类，也即一个个层\n",
    "            self._modules[block] = block  \n",
    "            # _modules是我们定义的一个特殊的成员变量，前面的下划线是为了命名冲突\n",
    "            # 从这个写法可以看出，它其实是一个有序字典，我们把传进来每个层既作为key也作为value存进该字典中\n",
    "            # 不过需要注意的是，value存的是它原来的类型，而key存的是它的字符串形式\n",
    "        # 其实除了用dict来存，也可以直接用list\n",
    "        # 另外，python3.6之后的dict默认是有序的\n",
    "            \n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():  # 这里是要对values进行遍历，而不是keys\n",
    "            # 每个block都是一个层，也即一个类\n",
    "            X = block(X)  # 将传入的input（即X）依次放入层中进行处理，并替换原来的值\n",
    "        return X    # 返回经过这些层处理过后的X\n",
    "    # 注意，X是经过拷贝的，这里传进来的X是拷贝后的，类外的X仍然不变\n",
    "    \n",
    "net3 = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "X2 = net2(X)\n",
    "print(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d5cec-70b7-4af6-8d54-7b12daa6768c",
   "metadata": {},
   "source": [
    "### 在正向传播函数中执行代码\n",
    "以下只是一个例子，告诉我们可以自由地定义forwad()，而这个例子本身没有什么实际作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575c17e6-4976-48dc-8368-042b94879738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0425, 0.3996, 0.0564, 0.7655, 0.1120, 0.1696, 0.5256, 0.5552, 0.5087,\n",
      "         0.8599, 0.6043, 0.1248, 0.8323, 0.0841, 0.4176, 0.6518, 0.7897, 0.5848,\n",
      "         0.1280, 0.0270],\n",
      "        [0.4337, 0.6388, 0.4869, 0.7589, 0.4771, 0.0353, 0.7225, 0.6173, 0.4926,\n",
      "         0.6505, 0.9257, 0.8357, 0.6483, 0.4285, 0.7880, 0.2571, 0.4744, 0.7820,\n",
      "         0.7323, 0.2980]])\n",
      "tensor(-0.2151, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20,20), requires_grad=False)\n",
    "        # 这行代码创建了一个形状为 (20, 20) 的随机权重矩阵，\n",
    "        # 并将其 requires_grad 属性设置为 False，意味着在反向传播时不会计算这个矩阵的梯度。\n",
    "        # 这个矩阵在网络的前向传播中被用作一个固定的（不学习的）权重。\n",
    "        self.linear = nn.Linear(20,20)\n",
    "        \n",
    "    # 对前向传播方法的自定义设计\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)  # X 先通过一个线性层\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)  # 再通过一个激活函数\n",
    "        # X 与前面定义的固定权重矩阵 self.rand_weight 进行矩阵乘法，结果加上1，最后通过ReLU激活函数。\n",
    "        # +1 相当于是做一下偏移吧\n",
    "        # 这里使用 torch.mm 进行矩阵乘法，但更简洁的方式是直接使用 X @ self.rand_weight（若PyTorch版本支持）\n",
    "        X = self.linear(X)  # 再通过一个线性层\n",
    "        while X.abs().sum() > 1:  \n",
    "            X /= 2\n",
    "        # 不断将 X 除以2，直到 X 的绝对值之和不大于1。\n",
    "        # 这是一个归一化步骤，但它与常见的归一化方法（如批归一化、层归一化等）不同，\n",
    "        # 因为它是在网络内部动态调整的，并且是基于整个 X 的绝对值之和。    \n",
    "        # 其实这个例子就是随便定义随便玩玩，没啥实际意义\n",
    "        \n",
    "        return X.sum()\n",
    "    \n",
    "    # 还有一个是反向传播，反向传播其实就是求导\n",
    "    \n",
    "net3 = FixedHiddenMLP()\n",
    "X2 = net3(X)\n",
    "print(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468a55f-2987-4f64-a1b8-061c13004a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 混合搭配各种组合块的方法\n",
    "以下就是一个随意嵌套的例子，也没有什么实际作用。只是告诉我们有多样的嵌套的可能。\n",
    "不过要注意的是，这一层的输入维度与输出之间、层与层之间的维度应该相等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a019403f-7ceb-482a-88f7-298d9bd2be55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0065, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20,64), nn.ReLU(), nn.Linear(64,32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32,16)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "    \n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16,20), FixedHiddenMLP())\n",
    "X2 = chimera(X)\n",
    "print(X2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
