*代码实现：[machine-learning/代码实现/knn.ipynb at master · HeXioahei/machine-learning (github.com)](https://github.com/HeXioahei/machine-learning/blob/master/%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/knn.ipynb)*

# 用途
对样本所属类型进行预测，是分类算法的一种。

# 算法原理
正所谓“近朱者赤近墨者黑”，和什么样的人相处，就会变成什么样的人。数据也是一样，和某一类型的数据离得越近，就越有可能是该类型的数据。

所以，在预测数据类型的问题上，就有一种叫KNN的算法，通过与近邻数据的比对，来预测其自身的类型。

# 预测的依据
判断的依据其实有两个，一个是，你离我越近，那我就越像你，另一个是，你在我附近出现的越多，那我就越像你。这两个因素都得考虑进去。

# 实现步骤
1. 计算群体中每个数据点与待测数据之间的”距离“
2. 根据”距离“的升序对数据点进行排序
3. 取排序后的前K个数据点，也就是离待预测数据最近的K个点
4. 对这K个距离进行加权平均，再根据所属标签进行求和。这一个步骤最为关键
	1. 首先，进行加权平均，距离越近的权值越大，这就体现了上文提到的第一个影响因素——越近越像。
	2. 其次，对属于同一类型标签的权重进行相加，存入该标签的总权重下，这就体现了上文提到的第二个影响因素——越多越像。
	3. 最后，将每个标签的权重进行比较，权重最大的标签即为待预测数据的标签。

# K的取值
K的选取十分关键，它影响着”越多越像“这一因素。不能太大，太大会导致分类模糊；也不能太小，太小的话会受个例的影响，使得波动较大，预测不准确。

那要具体要如何选取K呢？首先可以根据经验选一个适中的值，然后分别减小和增大，查看测试结果中正确率的变化。


