# 第一轮（11.04-11.17）

## 工作内容

学习self-attention、Transformer和ViT。复现论文中的代码并得到实验结果数据。代码是借用了b站某博主的代码，但我们结合ai的帮助对代码进行了反复多遍的理解和注释，不断加深了对ViT的理解。

## 期间遇到的难题

代码是借用他人的，拿过来用的时候一直运行不起来，一直卡在一个地方，无法前进，但是却又不报错，让人很摸不着头脑。排查问题排查了两三天，后来通过设置“断点”“调试”代码和查看他人评论，发现是gpu的问题，我们的gpu不支持多线程，所以只能用主线程来运行，最终修改了一下，就成功运行了。

## 成果收获

了解了ViT的原理，并通过学习借鉴他人的代码了解了基本的实现，有了一个基本的实现思路。

## 接下来的计划

在原有代码的基础上进行参数微调、数据集更换等，来看看有没有什么变化，找出模型存在的缺点，然后再针对缺点来想改进的方法。目前一直是在用自己的电脑跑，后面可能要考虑租算力了，不然很耗电脑寿命。

## 成员贡献

* 何佳进：制定计划，学习资料收集与分享，解释部分代码，解决代码运行问题，得到部分实验数据，整理实验报告总结。
* 黄艺玲：学习资料收集与分享，解决代码运行问题，进行大量实验得到大部分实验数据。
* 柯雨晴：学习资料分享，对代码进行详细的解释和注释，查询重要细节知识点，帮助大家理解代码、理解原理。
* 易敏亮：基础相对落后，这两周个人私事耽误，进度相对落后，但积极和小组成员探讨学习路径和方法，努力跟上进度，并为小组成员提供一些必要的帮助。


