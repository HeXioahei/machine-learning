{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedc1c08-1674-4e86-b7ce-36ee07ca6a6e",
   "metadata": {},
   "source": [
    "# 残差网络（ResNet）\n",
    "残差网络是卷积神经网络中十分重要、常用、好用且比较简单的一种网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a95468-bfbd-4fbd-bc6c-bec4f42e21a7",
   "metadata": {},
   "source": [
    "### 残差块\n",
    "\n",
    "（卷积层里的通道数是不是就对应着全连接层里的维度？）（好像是这样的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6d9865-2d0d-453b-a990-15a5e3b3d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    # 参数分别为：输入的通道数、输出的通道数、是否需要1×1的卷积层、如果要1×1的卷积层的话步幅为多大\n",
    "    def __init__(self, input_channels, num_channels, use_lxlconv=False, strides=1):\n",
    "        super().__init__()\n",
    "        # 第一个卷积层\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        # 第二个卷积层\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) # stride默认=1\n",
    "        if use_lxlconv:\n",
    "            # 1×1的卷积层，步长要与第一个卷积层的步长一致，才能保证最后可以相加\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        # 两个批量规范化层\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)  \n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        # 一个激活函数层\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # inplace=True 指原地进行操作，操作完成后覆盖原来的变量，节省内存，但会造成进行梯度回归失败\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        # 进入第一个卷积层，随即进入一个批量规范化层，又随即进入激活函数\n",
    "        \n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        # 进入第二个卷积层，随即批量规范化\n",
    "        \n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X             # 加上原数据\n",
    "        return F.relu(Y)   # 加完后再次激活并返回\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a69a80-e12f-4f25-9c22-2a1efa7d7a35",
   "metadata": {},
   "source": [
    "### 输入和输出形状一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5650e94-cf17-4a69-aecf-5ba839d6c65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk = Residual(3,3)  # 实例化一个输入和输出通道皆为3，不用1×1卷积，首个卷积步长为1（说明宽高都没有变化）的块\n",
    "X = torch.rand(4,3,6,6)  \n",
    "# 产生一个batch_size（批量大小）=4，channels（通道数）=3，h（高）=6，w（宽）=6 的数据输入。\n",
    "Y = blk(X)\n",
    "Y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d75a9-61fa-4d09-9d1e-49a9c0d3b7d5",
   "metadata": {},
   "source": [
    "### 增加输出通道数的同时，减半输出的宽和高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312fcaa6-3e1d-4e14-a995-26dd26131c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk2 = Residual(3, 6, use_lxlconv=True, strides=2)\n",
    "# 输入通道数为3，输出通道数加倍为6，同时步长加倍为2，使宽高减半。\n",
    "Y = blk2(X)\n",
    "Y.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a783a9-c5e2-4dae-9200-9d76548ab88f",
   "metadata": {},
   "source": [
    "### 只增加输出通道数，不改变宽高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08c0c91-d65a-437d-a7d9-9b445deadeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 6, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blk3 = Residual(3, 6, use_lxlconv=True, strides=1)\n",
    "Y = blk3(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bdebd-1a2d-40fa-ab38-833f34d34acb",
   "metadata": {},
   "source": [
    "### ResNet模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d0559f0-2432-4940-aaaa-8844870f342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，预处理部分的块 b1，和 GoogleNet 的 b1 是一样的\n",
    "# 一个7*7卷积层，一个批量规范化层，一个激活函数，一个3*3最大汇聚层\n",
    "b1 = nn.Sequential(nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),\n",
    "                   nn.BatchNorm2d(64),nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))\n",
    "\n",
    "# 定义大ResNet块\n",
    "# num_residuals 大ResNet块里有几个块\n",
    "# first_block=False 是否是第一个大ResNet块\n",
    "def resnet_block(input_channels, num_channels, num_residuals,  first_block=False):\n",
    "    blk = []  # 创建一个列表用来存放块\n",
    "    for i in range(num_residuals):\n",
    "        if i==0 and not first_block: \n",
    "            # 如果是第一个大ResNet块，且是该大ResNet块中的是第一个块，那么要在这个小块里对宽高减半\n",
    "            blk.append(Residual(input_channels, num_channels, use_lxlconv=True,strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk\n",
    "\n",
    "b2 = nn.Sequential(*resnet_block(64,64,2,first_block=True)) # 第一个大残差块\n",
    "b3 = nn.Sequential(*resnet_block(64,128,2))\n",
    "b4 = nn.Sequential(*resnet_block(128,256,2))\n",
    "b5 = nn.Sequential(*resnet_block(256,512,2))\n",
    "# 在预处理 b1 部分先进行一个宽高减半，在第一个大残差块那里不进行减半，后面的每一个大ResNet块的第一块都进行减半\n",
    "\n",
    "# 最后，与 GoogleNet 一样，加入一个全局平均汇聚层和全连接层\n",
    "net = nn.Sequential(b1,b2,b3,b4,b5,nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512,10))\n",
    "# net = nn.Sequential(b1,b2,b3,b4,b5,nn.Flatten(),nn.Linear(512,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668b7bc-7613-4d5d-9fee-08d56112384d",
   "metadata": {},
   "source": [
    "全局平均汇聚层的作用是在不改变通道数的情况下，将宽高都变为1，这样才能接入最后的全连接层。\n",
    "不太确定的话可以用上面cell最下面注释掉的那行代码来验证一下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d01cc43-e801-4854-9608-8d3ee4cabc0e",
   "metadata": {},
   "source": [
    "### 观察ResNet中不同模块的输入形状是如何变化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fc2f1e4-7b58-4137-a2fb-9d9582a572fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 64, 64])\n",
      "Sequential output shape:\t torch.Size([1, 64, 64, 64])\n",
      "Sequential output shape:\t torch.Size([1, 128, 32, 32])\n",
      "Sequential output shape:\t torch.Size([1, 256, 16, 16])\n",
      "Sequential output shape:\t torch.Size([1, 512, 8, 8])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 512, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 512])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1,1,256,256))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
