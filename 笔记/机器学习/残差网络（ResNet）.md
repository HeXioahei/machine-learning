我们总是企图加深神经网络来得到更精确的结果，但是加深不一定总能带来好处。随着网络的不断加深，函数的复杂度不断增大，其覆盖的范围不断扩大，但是，它不受控制，它扩大的方向可能并不是接近最优值的方向。

比如，假设我们所要寻找的最优值其实就在我们目前函数覆盖范围的上方，但是我神经网络的加深却是往下方不断加深的，这就逐渐偏离了正确方向。不断地加深反而越来越差。

如何使得加深不会偏离最优值。ResNet就给出了解决思路，即每一次加深都要求在原来函数的基础上进行扩充，新得到的函数必须包含原函数，就像是水中的涟漪不断从中心扩大，这样的话各个方向都可以被搜寻，就不会偏离，模型就不会随着网络的加深而变差。

所以，ResNet的核心思想就是，加更多的层时，不会使模型逐渐变差，不会偏离最优解。

那么具体是如何实现的呢？
其简易流程图如下：
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202410050935632.png)
这是ResNet的核心——残差块。概括来说就是`f(x)=x+g(x)`这样的一个结构。

